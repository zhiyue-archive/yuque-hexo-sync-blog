<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>噫吁戏</title>
  
  <subtitle>Just Do IT.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yixuxi.xyz/"/>
  <updated>2019-11-14T08:40:04.315Z</updated>
  <id>http://yixuxi.xyz/</id>
  
  <author>
    <name>zhiyue</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Regularization on GBDT</title>
    <link href="http://yixuxi.xyz/posts/hello-world_2/"/>
    <id>http://yixuxi.xyz/posts/hello-world_2/</id>
    <published>2019-11-14T08:40:04.315Z</published>
    <updated>2019-11-14T08:40:04.315Z</updated>
    
    <content type="html"><![CDATA[<script src="//gist.github.com/930f5f2f2b315ec967617d88600f16df.js"></script><p>之前一篇文章简单地讲了XGBoost的实现与普通GBDT实现的不同之处，本文尝试总结一下GBDT运用的正则化技巧。</p><script src="https://gist.github.com/zihengcat/1a80a31b671e5bb3db6eb7af5a4b30f7.js"></script><h3 id="Early-Stopping"><a href="#Early-Stopping" class="headerlink" title="Early Stopping"></a>Early Stopping</h3><p><a href="https://en.wikipedia.org/wiki/Early_stopping" target="_blank" rel="noopener">Early Stopping</a>是机器学习迭代式训练模型中很常见的防止过拟合技巧，维基百科里如下描述:</p><blockquote><p>In machine learning, early stopping is a form of <em>regularization</em> used to <em>avoid overfitting</em> when training a learner with an <em>iterative method</em>, such as gradient descent.</p></blockquote><p>具体的做法是选择一部分样本作为验证集，在迭代拟合训练集的过程中，如果模型在验证集里错误率不再下降，就停止训练，也就是说控制迭代的轮数（树的个数）。</p><p>XGBoost Python关于early stopping的<a href="https://github.com/dmlc/xgboost/blob/master/doc/python/python_intro.md#early-stopping" target="_blank" rel="noopener">参数设置文档</a>非常清晰，API如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># code snippets from xgboost python-package training.py</span><br><span class="line">def train(..., evals=(), early_stopping_rounds=None)</span><br><span class="line">&quot;&quot;&quot;Train a booster with given parameters.</span><br><span class="line">Parameters</span><br><span class="line">    ----------</span><br><span class="line">early_stopping_rounds: int</span><br><span class="line">        Activates early stopping. Validation error needs to decrease at least</span><br><span class="line">        every &lt;early_stopping_rounds&gt; round(s) to continue training.</span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure><p>Sklearn的GBDT实现虽然可以添加early stopping，但是比较复杂。官方没有相应的文档和代码样例，必须看<a href="https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/gradient_boosting.py#L931" target="_blank" rel="noopener">源码</a>。实现的时候需要用户提供monitor回调函数，且要了解源码内部_fit_stages函数的locals，总之对新手很不友好：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#code snippets from sklearn.ensemble.gradient_boosting</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaseGradientBoosting</span><span class="params">(six.with_metaclass<span class="params">(ABCMeta, BaseEnsemble,</span></span></span></span><br><span class="line"><span class="class"><span class="params"><span class="params">   _LearntSelectorMixin)</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Abstract base class for Gradient Boosting. """</span></span><br><span class="line">...</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y, sample_weight=None, monitor=None)</span>:</span></span><br><span class="line"><span class="string">"""Fit the gradient boosting model.</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">----------</span></span><br><span class="line"><span class="string">monitor : callable, optional</span></span><br><span class="line"><span class="string">            The monitor is called after each iteration with the current</span></span><br><span class="line"><span class="string">            iteration, a reference to the estimator and the local variables of</span></span><br><span class="line"><span class="string">            ``_fit_stages`` as keyword arguments ``callable(i, self,</span></span><br><span class="line"><span class="string">            locals())``. If the callable returns ``True`` the fitting procedure</span></span><br><span class="line"><span class="string">            is stopped. The monitor can be used for various things such as</span></span><br><span class="line"><span class="string">            computing held-out estimates, early stopping, model introspect, and</span></span><br><span class="line"><span class="string">            snapshoting.</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><p>对Sklearn感兴趣的可以看这篇文章<a href="*https://henri.io/posts/using-gradient-boosting-with-early-stopping.html">Using Gradient Boosting (with Early Stopping)</a>，里面有回调函数monitor的参考实现。</p><h3 id="Shrinkage"><a href="#Shrinkage" class="headerlink" title="Shrinkage"></a>Shrinkage</h3><p>Shrinkage就是将每棵树的输出结果乘一个因子($0&lt;\nu&lt;1$)，其中$\Sigma_{j=1}^{J_m}\gamma_{jm}I(x\in R_{jm})$是第m棵的输出，而$f(m)$是前m棵树的ensemble: $$f_m(x) = f_{m-1}(x) + \nu\cdot\Sigma_{j=1}^{J_m}\gamma_{jm}I(x\in R_{jm})$$<br>ESL书中这样讲：</p><blockquote><p>The parameter $\nu$ can be regarded as controlling the leanring rate of the boosting procedure</p></blockquote><p>$\nu$和迭代轮数M(树个数)是一个tradeoff，推荐的是$\nu$值设置小一点(如0.1)，而M设置大一些。这样一般能有比较好的准确率，代价是训练时间变长(与M成比例)。</p><p>下面是Sklearn的实现关于该参数设置的片段，XGBoost类似：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#code snippets from sklearn.ensemble.gradient_boosting</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GradientBoostingClassifier</span><span class="params">(BaseGradientBoosting, ClassifierMixin)</span>:</span></span><br><span class="line">    <span class="string">"""Gradient Boosting for classification."""</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ..., learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">100</span>, ...)</span>:</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">learning_rate : float, optional (default=0.1)</span></span><br><span class="line"><span class="string">        learning rate shrinks the contribution of each tree by `learning_rate`.</span></span><br><span class="line"><span class="string">        There is a trade-off between learning_rate and n_estimators.</span></span><br><span class="line"><span class="string">n_estimators : int (default=100)</span></span><br><span class="line"><span class="string">        The number of boosting stages to perform. Gradient boosting</span></span><br><span class="line"><span class="string">        is fairly robust to over-fitting so a large number usually</span></span><br><span class="line"><span class="string">        results in better performance</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><h3 id="Subsampling"><a href="#Subsampling" class="headerlink" title="Subsampling"></a>Subsampling</h3><p>Subsampling其实源于bootstrap averaging(bagging)思想，GBDT里的做法是在每一轮建树时，样本是从训练集合中无放回随机抽样的$\eta$部分，典型的$\eta$值是0.5。这样做既能对模型起正则作用，也能减少计算时间。</p><p>事实上，XGBoost和Sklearn的实现均借鉴了随机森林，除了有样本层次上的采样，也有特征采样。也就是说建树的时候只从随机选取的一些特征列寻找最优分裂。<br>下面是Sklearn里的相关参数设置的片段，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#code snippets from sklearn.ensemble.gradient_boosting</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GradientBoostingClassifier</span><span class="params">(BaseGradientBoosting, ClassifierMixin)</span>:</span></span><br><span class="line">    <span class="string">"""Gradient Boosting for classification."""</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ..., subsample=<span class="number">1.0</span>, max_features=None,...)</span>:</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">subsample : float, optional (default=1.0)</span></span><br><span class="line"><span class="string">        The fraction of samples to be used for fitting the individual base</span></span><br><span class="line"><span class="string">        learners. If smaller than 1.0 this results in Stochastic Gradient</span></span><br><span class="line"><span class="string">        Boosting. `subsample` interacts with the parameter `n_estimators`.</span></span><br><span class="line"><span class="string">        Choosing `subsample &lt; 1.0` leads to a reduction of variance</span></span><br><span class="line"><span class="string">        and an increase in bias.</span></span><br><span class="line"><span class="string">    max_features : int, float, string or None, optional (default=None)</span></span><br><span class="line"><span class="string">        The number of features to consider when looking for the best split:</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><h3 id="Regularized-Learning-Objective"><a href="#Regularized-Learning-Objective" class="headerlink" title="Regularized Learning Objective"></a>Regularized Learning Objective</h3><p>将树模型的复杂度作为正则项显式地加进优化目标里，是XGBoost实现的独到之处。<br>$$\mathcal{L}^{(t)} = \sum_{i=1}^n l(y_i, y_i^{*(t-1)} + f_t(\mathrm{x}_i)) + \Omega(f_t)$$<br>where<br>$$ \Omega(f)=\gamma T+ \frac{1}{2}\lambda||w||^2$$</p><p>其中$y_i^{*(t)}$是第t轮第i个instance的预测值，$f_t$是第t轮建的树，$T$是树叶结点数目，$w$是树叶结点的输出，$\gamma, \lambda$是正则化参数。深入了解加了正则后如何推导剃度更新的可以看XGBoost的<a href="http://arxiv.org/abs/1603.02754" target="_blank" rel="noopener">论文</a>。</p><p>我个人的看法是将树模型的复杂度作为正则化项加在优化目标，相比自己通过参数控制每轮树的复杂度更直接，这可能是XGBoost相比普通GBDT实现效果更好的一个很重要的原因。很遗憾，Sklearn暂时无相应的实现。</p><h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>Dropout是deep learning里很常用的正则化技巧，很自然的我们会想能不能把Dropout用到GBDT模型上呢？<a href="http://www.jmlr.org/proceedings/papers/v38/" target="_blank" rel="noopener">AISTATS2015</a>有篇文章[DART: Dropouts meet Multiple Additive Regression Trees](DART: Dropouts meet Multiple Additive Regression Trees)进行了一些尝试。</p><p>文中提到GBDT里会出现<em>over-specialization</em>的问题：</p><blockquote><p>Trees added at later iterations tend to impact the prediction of only<br>a few instances, and they make negligible contribution<br>towards the prediction of all the remaining instances.<br>We call this issue of subsequent trees affecting the prediction of only<br>a small fraction of the training instances <em>over-specialization</em>.</p></blockquote><p>也就是说前面迭代的树对预测值的贡献比较大，后面的树会集中预测一小部分样本的偏差。Shrinkage可以减轻<em>over-specialization</em>的问题，但不是很好。作者想通过Dropout来平衡所有树对预测的贡献，如下图的效果：<br><img src="/assets/images/gbdt_dart.png" alt=""></p><p>具体的做法如下：</p><blockquote><p>DART divergesfrom MART at two places. First, when computing the<br>gradient that the next tree will fit, only a random subset<br>of the existing ensemble is considered.<br>The second place at which DART diverges from MART<br>is when adding the new tree to the ensemble where<br>DART performs a normalization step.</p></blockquote><p>简单说就是每次新加一棵树，这棵树要拟合的并不是之前全部树ensemble后的残差，而是随机抽取的一些树ensemble；同时新加的树结果要规范化一下。</p><p>这种新做法对GBDT效果的提升有多明显还有待大家探索尝试。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script src=&quot;//gist.github.com/930f5f2f2b315ec967617d88600f16df.js&quot;&gt;&lt;/script&gt;


&lt;p&gt;之前一篇文章简单地讲了XGBoost的实现与普通GBDT实现的不同之处，本文尝试总结一下GBDT运用的正则化技巧。
      
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yixuxi.xyz/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>how to learn golang</title>
    <link href="http://yixuxi.xyz/posts/yuque/how%20to%20learn%20golang/"/>
    <id>http://yixuxi.xyz/posts/yuque/how to learn golang/</id>
    <published>2019-11-14T00:36:18.000Z</published>
    <updated>2019-11-14T08:40:44.739Z</updated>
    
    <content type="html"><![CDATA[<ul><li>go.dev</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;go.dev&lt;/li&gt;
&lt;/ul&gt;

      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>博客第1期</title>
    <link href="http://yixuxi.xyz/posts/yuque/%E5%8D%9A%E5%AE%A2%E7%AC%AC1%E6%9C%9F/"/>
    <id>http://yixuxi.xyz/posts/yuque/博客第1期/</id>
    <published>2019-11-12T22:23:04.000Z</published>
    <updated>2019-11-14T08:40:44.743Z</updated>
    
    <content type="html"><![CDATA[<p>该博客模版案例来自 <a href="#">@蚂蚁金服体验技术部</a><br><a name="Cpuzj"></a></p><h3 id=""><a href="#" class="headerlink" title=""></a></h3><p>测试<br><a name="az6uF"></a></p><h3 id="插入带链接的标题（ctrl-K）Best-Year-in-Music"><a href="#插入带链接的标题（ctrl-K）Best-Year-in-Music" class="headerlink" title="插入带链接的标题（ctrl+K）Best Year in Music"></a>插入带链接的标题（ctrl+K）<a href="https://pudding.cool/projects/music-history/" target="_blank" rel="noopener">Best Year in Music</a></h3><p>这幅可视化作品展示了 1960 年以来，Billboard 历年榜单的前五名音乐的变化情况，效果呈现设计感十足，还可以在聆听音乐的同时回顾音乐的变迁史。<br /><img src="https://cdn.nlark.com/yuque/0/2019/png/85075/1569743511172-7dfff3da-684a-4902-babc-cd9117fe3b7a.png#align=left&display=inline&height=648&name=image.png&originHeight=1296&originWidth=2868&search=&size=965213&status=done&width=1434" alt="image.png"><br /><em>插入图片，点击图片，设置图片大小</em><br />值得一提的是，作者正是大名鼎鼎的数据新闻站点 <a href="https://pudding.cool/" target="_blank" rel="noopener">The Pudding</a>。作为可视化新闻媒体中的姣姣者，The Pudding 擅长对那些兼具信息量和娱乐度的争议性话题进行可视化呈现，来使得复杂的观点变得更容易被公众理解。该团队虽然仅由数名全职记者和工程师组成，但战斗力强悍，目前已有数十个大型的数据可视化作品。<br />输入作者 <a href="/dengfuping">@诸岳(dengfuping)</a><br /><br><br /></p><p><a name="xyPrr"></a></p><h3 id="Road-Suffixes-in-the-USA"><a href="#Road-Suffixes-in-the-USA" class="headerlink" title="Road Suffixes in the USA"></a><a href="https://erdavis.com/2019/07/04/road-suffixes-in-the-usa-take-2/" target="_blank" rel="noopener">Road Suffixes in the USA</a></h3><p>美国道路命名多种多样，例如著名的纽约第五大道（Fifth Avenue），又或者凤凰城（Phoenix）的Washington Street，Thomas Road，Union Hill Drive等。不同的道路，有不同的叫法，Avenue, Street, Road, Drive等等。想了解更多关于美国道路命名可以参考<a href="http://blog.sina.com.cn/s/blog_7010d1db0101he93.html" target="_blank" rel="noopener">这篇文章</a>。<br />作者用R语言统计了美国每个县的每种道路后缀（例如Avenue）的总里程，并把每个县总里程最大的道路后缀通过颜色映射到地图上。可以看出美国大部分县的道路都是以Rd(Road的缩写)结尾的。<br /><img src="https://cdn.nlark.com/yuque/0/2019/png/85075/1569743625739-f1c8b376-5632-4d08-8fcc-56142f0b0c1a.png#align=left&display=inline&height=603&name=road-suffix-map.png&originHeight=1767&originWidth=2048&search=&size=328604&status=done&width=699" alt="road-suffix-map.png"><br /><em>插入图片，点击图片，设置图片大小</em></p><p>另外还以柱状图的方式展示了每种道路后缀的总里程（英里）：<br /><img src="https://cdn.nlark.com/yuque/0/2019/png/85075/1569743625720-b8759449-4b6a-4b11-bbc0-896fe8dca73e.png#align=left&display=inline&height=737&name=road-suffix-bar.png&originHeight=737&originWidth=890&search=&size=23572&status=done&width=890" alt="road-suffix-bar.png"><br />by <a href="/changzhe">@长哲(changzhe)</a></p><p><a name="hduVH"></a></p><h1 id="「加餐」Hello-World-图可视化"><a href="#「加餐」Hello-World-图可视化" class="headerlink" title="「加餐」Hello World 图可视化"></a><a href="https://zhuanlan.zhihu.com/p/83685690" target="_blank" rel="noopener">「</a><a href="https://zhuanlan.zhihu.com/p/83685690" target="_blank" rel="noopener">加餐」</a><a href="https://zhuanlan.zhihu.com/p/83685690" target="_blank" rel="noopener">Hello World 图可视化</a></h1><p>图可视化是信息可视化的子领域，它通过展示元素、关系，帮助用户获取数据的洞悉能力。它已被广泛地应用在流程图、社交网络、英特网、蛋白质网络等关系数据的呈现。<br /><a href="https://zhuanlan.zhihu.com/p/83685690" target="_blank" rel="noopener">本文</a>由 AntV 团队 <a href="/shiwu-5wap2">@十吾(shiwu-5wap2)</a> 👆撰写，参考自[1][2]两篇图可视化学术论文，简要介绍图可视化的历史、背景、机遇与挑战。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;该博客模版案例来自 &lt;a href=&quot;#&quot;&gt;@蚂蚁金服体验技术部&lt;/a&gt;&lt;br&gt;&lt;a name=&quot;Cpuzj&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;&quot;&gt;&lt;a href=&quot;#&quot; class=&quot;headerlink&quot; title=&quot;&quot;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;测试&lt;br&gt;&lt;a n
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yixuxi.xyz/posts/hello-world/"/>
    <id>http://yixuxi.xyz/posts/hello-world/</id>
    <published>2016-04-05T06:16:00.000Z</published>
    <updated>2019-11-14T08:40:04.315Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
      <category term="math" scheme="http://yixuxi.xyz/categories/math/"/>
    
    
  </entry>
  
</feed>

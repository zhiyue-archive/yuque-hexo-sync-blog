<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>噫吁戏</title>
  
  <subtitle>Just Do IT.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yixuxi.xyz/"/>
  <updated>2019-11-04T01:59:48.923Z</updated>
  <id>http://yixuxi.xyz/</id>
  
  <author>
    <name>zhiyue</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Regularization on GBDT</title>
    <link href="http://yixuxi.xyz/posts/hello-world_2/"/>
    <id>http://yixuxi.xyz/posts/hello-world_2/</id>
    <published>2019-11-04T01:59:48.923Z</published>
    <updated>2019-11-04T01:59:48.923Z</updated>
    
    <content type="html"><![CDATA[<script src="//gist.github.com/930f5f2f2b315ec967617d88600f16df.js"></script><p>之前一篇文章简单地讲了XGBoost的实现与普通GBDT实现的不同之处，本文尝试总结一下GBDT运用的正则化技巧。</p><script src="https://gist.github.com/zihengcat/1a80a31b671e5bb3db6eb7af5a4b30f7.js"></script><h3 id="Early-Stopping"><a href="#Early-Stopping" class="headerlink" title="Early Stopping"></a>Early Stopping</h3><p><a href="https://en.wikipedia.org/wiki/Early_stopping" target="_blank" rel="noopener">Early Stopping</a>是机器学习迭代式训练模型中很常见的防止过拟合技巧，维基百科里如下描述:</p><blockquote><p>In machine learning, early stopping is a form of <em>regularization</em> used to <em>avoid overfitting</em> when training a learner with an <em>iterative method</em>, such as gradient descent.</p></blockquote><p>具体的做法是选择一部分样本作为验证集，在迭代拟合训练集的过程中，如果模型在验证集里错误率不再下降，就停止训练，也就是说控制迭代的轮数（树的个数）。</p><p>XGBoost Python关于early stopping的<a href="https://github.com/dmlc/xgboost/blob/master/doc/python/python_intro.md#early-stopping" target="_blank" rel="noopener">参数设置文档</a>非常清晰，API如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># code snippets from xgboost python-package training.py</span><br><span class="line">def train(..., evals=(), early_stopping_rounds=None)</span><br><span class="line">&quot;&quot;&quot;Train a booster with given parameters.</span><br><span class="line">Parameters</span><br><span class="line">    ----------</span><br><span class="line">early_stopping_rounds: int</span><br><span class="line">        Activates early stopping. Validation error needs to decrease at least</span><br><span class="line">        every &lt;early_stopping_rounds&gt; round(s) to continue training.</span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure><p>Sklearn的GBDT实现虽然可以添加early stopping，但是比较复杂。官方没有相应的文档和代码样例，必须看<a href="https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/gradient_boosting.py#L931" target="_blank" rel="noopener">源码</a>。实现的时候需要用户提供monitor回调函数，且要了解源码内部_fit_stages函数的locals，总之对新手很不友好：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#code snippets from sklearn.ensemble.gradient_boosting</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaseGradientBoosting</span><span class="params">(six.with_metaclass<span class="params">(ABCMeta, BaseEnsemble,</span></span></span></span><br><span class="line"><span class="class"><span class="params"><span class="params">   _LearntSelectorMixin)</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Abstract base class for Gradient Boosting. """</span></span><br><span class="line">...</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y, sample_weight=None, monitor=None)</span>:</span></span><br><span class="line"><span class="string">"""Fit the gradient boosting model.</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">----------</span></span><br><span class="line"><span class="string">monitor : callable, optional</span></span><br><span class="line"><span class="string">            The monitor is called after each iteration with the current</span></span><br><span class="line"><span class="string">            iteration, a reference to the estimator and the local variables of</span></span><br><span class="line"><span class="string">            ``_fit_stages`` as keyword arguments ``callable(i, self,</span></span><br><span class="line"><span class="string">            locals())``. If the callable returns ``True`` the fitting procedure</span></span><br><span class="line"><span class="string">            is stopped. The monitor can be used for various things such as</span></span><br><span class="line"><span class="string">            computing held-out estimates, early stopping, model introspect, and</span></span><br><span class="line"><span class="string">            snapshoting.</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><p>对Sklearn感兴趣的可以看这篇文章<a href="*https://henri.io/posts/using-gradient-boosting-with-early-stopping.html">Using Gradient Boosting (with Early Stopping)</a>，里面有回调函数monitor的参考实现。</p><h3 id="Shrinkage"><a href="#Shrinkage" class="headerlink" title="Shrinkage"></a>Shrinkage</h3><p>Shrinkage就是将每棵树的输出结果乘一个因子($0&lt;\nu&lt;1$)，其中$\Sigma_{j=1}^{J_m}\gamma_{jm}I(x\in R_{jm})$是第m棵的输出，而$f(m)$是前m棵树的ensemble: $$f_m(x) = f_{m-1}(x) + \nu\cdot\Sigma_{j=1}^{J_m}\gamma_{jm}I(x\in R_{jm})$$<br>ESL书中这样讲：</p><blockquote><p>The parameter $\nu$ can be regarded as controlling the leanring rate of the boosting procedure</p></blockquote><p>$\nu$和迭代轮数M(树个数)是一个tradeoff，推荐的是$\nu$值设置小一点(如0.1)，而M设置大一些。这样一般能有比较好的准确率，代价是训练时间变长(与M成比例)。</p><p>下面是Sklearn的实现关于该参数设置的片段，XGBoost类似：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#code snippets from sklearn.ensemble.gradient_boosting</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GradientBoostingClassifier</span><span class="params">(BaseGradientBoosting, ClassifierMixin)</span>:</span></span><br><span class="line">    <span class="string">"""Gradient Boosting for classification."""</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ..., learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">100</span>, ...)</span>:</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">learning_rate : float, optional (default=0.1)</span></span><br><span class="line"><span class="string">        learning rate shrinks the contribution of each tree by `learning_rate`.</span></span><br><span class="line"><span class="string">        There is a trade-off between learning_rate and n_estimators.</span></span><br><span class="line"><span class="string">n_estimators : int (default=100)</span></span><br><span class="line"><span class="string">        The number of boosting stages to perform. Gradient boosting</span></span><br><span class="line"><span class="string">        is fairly robust to over-fitting so a large number usually</span></span><br><span class="line"><span class="string">        results in better performance</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><h3 id="Subsampling"><a href="#Subsampling" class="headerlink" title="Subsampling"></a>Subsampling</h3><p>Subsampling其实源于bootstrap averaging(bagging)思想，GBDT里的做法是在每一轮建树时，样本是从训练集合中无放回随机抽样的$\eta$部分，典型的$\eta$值是0.5。这样做既能对模型起正则作用，也能减少计算时间。</p><p>事实上，XGBoost和Sklearn的实现均借鉴了随机森林，除了有样本层次上的采样，也有特征采样。也就是说建树的时候只从随机选取的一些特征列寻找最优分裂。<br>下面是Sklearn里的相关参数设置的片段，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#code snippets from sklearn.ensemble.gradient_boosting</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GradientBoostingClassifier</span><span class="params">(BaseGradientBoosting, ClassifierMixin)</span>:</span></span><br><span class="line">    <span class="string">"""Gradient Boosting for classification."""</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ..., subsample=<span class="number">1.0</span>, max_features=None,...)</span>:</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">subsample : float, optional (default=1.0)</span></span><br><span class="line"><span class="string">        The fraction of samples to be used for fitting the individual base</span></span><br><span class="line"><span class="string">        learners. If smaller than 1.0 this results in Stochastic Gradient</span></span><br><span class="line"><span class="string">        Boosting. `subsample` interacts with the parameter `n_estimators`.</span></span><br><span class="line"><span class="string">        Choosing `subsample &lt; 1.0` leads to a reduction of variance</span></span><br><span class="line"><span class="string">        and an increase in bias.</span></span><br><span class="line"><span class="string">    max_features : int, float, string or None, optional (default=None)</span></span><br><span class="line"><span class="string">        The number of features to consider when looking for the best split:</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><h3 id="Regularized-Learning-Objective"><a href="#Regularized-Learning-Objective" class="headerlink" title="Regularized Learning Objective"></a>Regularized Learning Objective</h3><p>将树模型的复杂度作为正则项显式地加进优化目标里，是XGBoost实现的独到之处。<br>$$\mathcal{L}^{(t)} = \sum_{i=1}^n l(y_i, y_i^{*(t-1)} + f_t(\mathrm{x}_i)) + \Omega(f_t)$$<br>where<br>$$ \Omega(f)=\gamma T+ \frac{1}{2}\lambda||w||^2$$</p><p>其中$y_i^{*(t)}$是第t轮第i个instance的预测值，$f_t$是第t轮建的树，$T$是树叶结点数目，$w$是树叶结点的输出，$\gamma, \lambda$是正则化参数。深入了解加了正则后如何推导剃度更新的可以看XGBoost的<a href="http://arxiv.org/abs/1603.02754" target="_blank" rel="noopener">论文</a>。</p><p>我个人的看法是将树模型的复杂度作为正则化项加在优化目标，相比自己通过参数控制每轮树的复杂度更直接，这可能是XGBoost相比普通GBDT实现效果更好的一个很重要的原因。很遗憾，Sklearn暂时无相应的实现。</p><h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>Dropout是deep learning里很常用的正则化技巧，很自然的我们会想能不能把Dropout用到GBDT模型上呢？<a href="http://www.jmlr.org/proceedings/papers/v38/" target="_blank" rel="noopener">AISTATS2015</a>有篇文章[DART: Dropouts meet Multiple Additive Regression Trees](DART: Dropouts meet Multiple Additive Regression Trees)进行了一些尝试。</p><p>文中提到GBDT里会出现<em>over-specialization</em>的问题：</p><blockquote><p>Trees added at later iterations tend to impact the prediction of only<br>a few instances, and they make negligible contribution<br>towards the prediction of all the remaining instances.<br>We call this issue of subsequent trees affecting the prediction of only<br>a small fraction of the training instances <em>over-specialization</em>.</p></blockquote><p>也就是说前面迭代的树对预测值的贡献比较大，后面的树会集中预测一小部分样本的偏差。Shrinkage可以减轻<em>over-specialization</em>的问题，但不是很好。作者想通过Dropout来平衡所有树对预测的贡献，如下图的效果：<br><img src="/assets/images/gbdt_dart.png" alt=""></p><p>具体的做法如下：</p><blockquote><p>DART divergesfrom MART at two places. First, when computing the<br>gradient that the next tree will fit, only a random subset<br>of the existing ensemble is considered.<br>The second place at which DART diverges from MART<br>is when adding the new tree to the ensemble where<br>DART performs a normalization step.</p></blockquote><p>简单说就是每次新加一棵树，这棵树要拟合的并不是之前全部树ensemble后的残差，而是随机抽取的一些树ensemble；同时新加的树结果要规范化一下。</p><p>这种新做法对GBDT效果的提升有多明显还有待大家探索尝试。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script src=&quot;//gist.github.com/930f5f2f2b315ec967617d88600f16df.js&quot;&gt;&lt;/script&gt;


&lt;p&gt;之前一篇文章简单地讲了XGBoost的实现与普通GBDT实现的不同之处，本文尝试总结一下GBDT运用的正则化技巧。
      
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yixuxi.xyz/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>测试数学公式</title>
    <link href="http://yixuxi.xyz/posts/yuque/%E6%B5%8B%E8%AF%95%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/"/>
    <id>http://yixuxi.xyz/posts/yuque/测试数学公式/</id>
    <published>2019-11-03T17:47:09.000Z</published>
    <updated>2019-11-04T02:00:30.416Z</updated>
    
    <content type="html"><![CDATA[<ul><li>测试行内公式</li></ul><p>回随机抽样的<img src="https://cdn.nlark.com/yuque/__latex/ffe9f913124f345732e9f00fa258552e.svg#card=math&code=%5Ceta&height=13&width=7" alt="">)部分，典型的<img src="https://cdn.nlark.com/yuque/__latex/ffe9f913124f345732e9f00fa258552e.svg#card=math&code=%5Ceta&height=13&width=7" alt="">值是0.5</p><ul><li>测试公式</li></ul><p><img src="https://cdn.nlark.com/yuque/__latex/68c035dfbb544bd6a17d52f4593155ee.svg#card=math&code=%5Cmathcal%7BL%7D%5E%7B%28t%29%7D%20%3D%20%5Csum_%7Bi%3D1%7D%5En%20l%28y_i%2C%20y_i%5E%7B%2A%28t-1%29%7D%20%2B%20f_t%28%5Cmathrm%7Bx%7D_i%29%29%20%2B%20%5COmega%28f_t%29%24%24%20where%20%24%24%20%5COmega%28f%29%3D%5Cgamma%20T%2B%20%5Cfrac%7B1%7D%7B2%7D%5Clambda%7C%7Cw%7C%7C%5E2&height=44&width=465" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;测试行内公式&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;回随机抽样的&lt;img src=&quot;https://cdn.nlark.com/yuque/__latex/ffe9f913124f345732e9f00fa258552e.svg#card=math&amp;code=%5Ceta
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>猪猪是个傻猪猪</title>
    <link href="http://yixuxi.xyz/posts/yuque/%E7%8C%AA%E7%8C%AA%E6%98%AF%E4%B8%AA%E5%82%BB%E7%8C%AA%E7%8C%AA/"/>
    <id>http://yixuxi.xyz/posts/yuque/猪猪是个傻猪猪/</id>
    <published>2019-11-03T07:54:25.000Z</published>
    <updated>2019-11-04T02:00:30.416Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>测试自动同步</title>
    <link href="http://yixuxi.xyz/posts/yuque/%E6%B5%8B%E8%AF%95%E8%87%AA%E5%8A%A8%E5%90%8C%E6%AD%A5/"/>
    <id>http://yixuxi.xyz/posts/yuque/测试自动同步/</id>
    <published>2019-11-03T07:40:40.000Z</published>
    <updated>2019-11-04T02:00:30.416Z</updated>
    
    <content type="html"><![CDATA[<p>猪猪猪组猪猪组暂住证组织者组织</p><p>是金佛我家佛喊我哈佛回房间后方宏伟哈佛红围巾<br />佛耳温计foe就无法为合法和佛IE哈佛IE见覅而绝非复合肥hi耳机</p><p>无法全额而非</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;猪猪猪组猪猪组暂住证组织者组织&lt;/p&gt;
&lt;p&gt;是金佛我家佛喊我哈佛回房间后方宏伟哈佛红围巾&lt;br /&gt;佛耳温计foe就无法为合法和佛IE哈佛IE见覅而绝非复合肥hi耳机&lt;/p&gt;
&lt;p&gt;无法全额而非&lt;/p&gt;

      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>如何学习数据科学</title>
    <link href="http://yixuxi.xyz/posts/yuque/%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/"/>
    <id>http://yixuxi.xyz/posts/yuque/如何学习数据科学/</id>
    <published>2019-07-22T08:08:11.000Z</published>
    <updated>2019-11-04T02:00:30.412Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>如何学习 pandas</title>
    <link href="http://yixuxi.xyz/posts/yuque/%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0%20pandas/"/>
    <id>http://yixuxi.xyz/posts/yuque/如何学习 pandas/</id>
    <published>2019-07-22T08:07:44.000Z</published>
    <updated>2019-11-04T02:00:30.412Z</updated>
    
    <content type="html"><![CDATA[<p><a name="eDcbx"></a></p><h2 id="相关参考资料"><a href="#相关参考资料" class="headerlink" title="相关参考资料"></a>相关参考资料</h2><p>hello pandas <a href="https://www.kaggle.com/colinmorris/hello-python" target="_blank" rel="noopener">https://www.kaggle.com/colinmorris/hello-python</a><br />pandas cards <a href="https://assets.datacamp.com/blog_assets/PandasPythonForDataScience.pdf" target="_blank" rel="noopener">https://assets.datacamp.com/blog_assets/PandasPythonForDataScience.pdf</a><br /><a href="https://www.kaggle.com/residentmario/creating-reading-and-writing" target="_blank" rel="noopener">https://www.kaggle.com/residentmario/creating-reading-and-writing</a><br /><a href="https://www.kaggle.com/jabari/exercise-explore-your-data/edit" target="_blank" rel="noopener">https://www.kaggle.com/jabari/exercise-explore-your-data/edit</a><br /><a href="https://www.kaggle.com/dansbecker/basic-data-exploration" target="_blank" rel="noopener">https://www.kaggle.com/dansbecker/basic-data-exploration</a><br /><a href="https://wiki.python.org/moin/BeginnersGuide/NonProgrammers" target="_blank" rel="noopener">https://wiki.python.org/moin/BeginnersGuide/NonProgrammers</a><br><a name="770Sw"></a></p><h2 id="相关练习"><a href="#相关练习" class="headerlink" title="相关练习"></a>相关练习</h2><pre><code>titanic [https://www.kaggle.com/c/titanic](https://www.kaggle.com/c/titanic)</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a name=&quot;eDcbx&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;相关参考资料&quot;&gt;&lt;a href=&quot;#相关参考资料&quot; class=&quot;headerlink&quot; title=&quot;相关参考资料&quot;&gt;&lt;/a&gt;相关参考资料&lt;/h2&gt;&lt;p&gt;hello pandas &lt;a href=&quot;http
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>《机器学习》西瓜书阅读笔记 | SF-Zhou&amp;#39;s Blog</title>
    <link href="http://yixuxi.xyz/posts/yuque/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%20%7C%20SF-Zhou&#39;s%20Blog/"/>
    <id>http://yixuxi.xyz/posts/yuque/《机器学习》西瓜书阅读笔记 | SF-Zhou&#39;s Blog/</id>
    <published>2019-04-03T02:31:13.000Z</published>
    <updated>2019-11-04T02:00:30.404Z</updated>
    
    <content type="html"><![CDATA[<h2 id="第-1-章-绪论"><a href="#第-1-章-绪论" class="headerlink" title="第 1 章 绪论"></a>第 1 章 绪论</h2><h3 id="基本术语"><a href="#基本术语" class="headerlink" title="基本术语"></a>基本术语</h3><p><code>机器学习</code>：在计算机上从<code>数据</code>（data）中产生<code>模型</code>（model）的算法，即<code>学习算法</code>（learning algorithm）。</p><p>A computer program is said to learn from experience EEE with respect to some class of tasks TTT and performance measure PPP, if its performance at tasks in TTT, as measured by PPP, improves with experience EEE.</p><p>一般地，令 D={x⃗1,x⃗2,⋯,x⃗m}D = \left \{ \vec {x}_1, \vec {x}_2, \cdots, \vec {x}_m \right \}D={x1​,x2​,⋯,xm​} 表示包含 mmm 个<code>样本</code>（sample）的数据集，每个示例由 ddd 个<code>属性</code>（attribute）描述，则每个样本 x⃗i={xi1;xi2;⋯;xid}\vec x_i = \left \{x_{i1}; x_{i2}; \cdots; x_{id} \right \}xi​={xi1​;xi2​;⋯;xid​} 是 ddd 维样本空间 X\mathcal{X}X 中的一个向量，x⃗i∈X\vec x_i \in \mathcal{X}xi​∈X，其中 xijx_{ij}xij​ 是 x⃗i\vec x_ixi​ 在第 jjj 个属性上的取值，ddd 称为样本 x⃗i\vec x_ixi​ 的<code>维数</code>（dimensionality）。</p><p>属性张成的空间称为<code>样本空间</code>（sample space），每个样本都可在这个空间中找到唯一的坐标位置，因此也把一个样本称为一个<code>特征向量</code>（feature vector）。</p><p>从数据中学得模型的过程称之为<code>学习</code>（learning）或<code>训练</code>（training），学得模型适用于新样本的能力称为<code>泛化</code>（generalization）能力。</p><h3 id="假设空间"><a href="#假设空间" class="headerlink" title="假设空间"></a>假设空间</h3><p><code>归纳</code>（induction）与<code>演绎</code>（deduction）是科学推理的两大基本手段。前者是从特殊到一般的泛化（generalization）过程，后者是从一般到特殊的特化（specialization）过程。从样例中学习是一个归纳的过程，亦称<code>归纳学习</code>（inductive learning）。</p><p>狭义的归纳学习是从数据中学得<code>概念</code>（concept），最基本的概念学习是布尔概念学习。可以把学习的过程看作一个在所有<code>假设</code>（hypothesis）组成的空间中进行搜索的过程，搜索目标是找到与训练集<code>匹配</code>（fit）的假设。</p><p>假设的表示一旦确定，<code>假设空间</code>（hypothesis space）及其规模大小就确定了。现实问题中通常面临很大的假设空间，但样本训练集是有限的，因此可能有多个假设与训练集一致，即存在一个与训练集一致的假设集合，称之为<code>版本空间</code>（version space）。</p><h3 id="归纳偏好"><a href="#归纳偏好" class="headerlink" title="归纳偏好"></a>归纳偏好</h3><p>机器学习算法在学习过程中对某种类型假设的偏好，称为<code>归纳偏好</code>（inductive bias）。归纳偏好可看作是学习算法在庞大的假设空间中对假设进行选择的价值观。</p><p><code>奥卡姆剃刀</code>（Occam’s Razor）是自然科学研究中常用的原则，即若存在多个假设与观察一致，则选最简单的那个。如无必要，勿增实体。</p><p>但奥卡姆剃刀原则并不平凡，“简单”的评价标准无法量化。事实上归纳偏好对应了学习算法本身所做出的关于“什么样的模型更好”的假设。<code>没有免费的午餐定理</code>（No Free Lunch Theorem，NFL）证明了在真实目标函数 fff 均匀分布的情况下，所有学习算法学得的模型期望性能是一致的。</p><p>脱离实际问题，空谈“什么学习算法更好”毫无意义。</p><h2 id="第-2-章-模型评估与选择"><a href="#第-2-章-模型评估与选择" class="headerlink" title="第 2 章 模型评估与选择"></a>第 2 章 模型评估与选择</h2><h3 id="经验误差与过拟合"><a href="#经验误差与过拟合" class="headerlink" title="经验误差与过拟合"></a>经验误差与过拟合</h3><p>学习器的实际输出与样本的真实输出之间的差异称为<code>误差</code>（error），训练集上的误差称为<code>训练误差</code>（training error），新样本上的误差称为<code>泛化误差</code>（generalization error）。</p><p>为了使泛化误差最小化，应该从训练样本中尽可能学出适用于所有潜在样本的“普遍规律”。而将训练样本的特点当作了所有潜在样本的一般性质，导致泛化性能下降的现象，称为<code>过拟合</code>（overfitting），相对地没有充分习得训练样本的一般性质的现象，称为<code>欠拟合</code>（underfitting）。</p><p>现实任务中，存在多种学习算法、不同参数配置，产生不同的模型，需要选择其中合适的模型，该问题称为<code>模型选择</code>（model selection）问题。理想状态下使用泛化误差作为模型选择的评价标准，但泛化误差无法直接获得。</p><h3 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h3><p>通常使用<code>测试集</code>（testing set）来测试学习器对新样本的判别能力，以测试集上的<code>测试误差</code>（testing error）作为泛化误差的近似。通常假设测试样本是从样本真实分布中独立同分布采样而得。</p><p>对于包含 mmm 个样本的数据集 D={(x⃗1,y1),(x⃗2,y2),⋯,(x⃗m,ym)}D = \left \{ (\vec x_1, y_1), (\vec x_2, y_2), \cdots, (\vec x_m, y_m) \right \}D={(x1​,y1​),(x2​,y2​),⋯,(xm​,ym​)}，需要将其分解为训练集 SSS、验证集 VVV 和测试集 TTT，常用的方法有留出法、交叉验证法和<code>自助法</code>（bootstrapping）。</p><p>自助法即从数据集中进行 mmm 次可重复采样，可以选出约 36.8% 的样本作为测试集，在数据集较小时较为有效。</p><p>机器学习常涉及两类参数：一是算法的参数，称为<code>超参数</code>（hyper parameter），一是模型的参数。对超参数进行设定调优的过程称为<code>调参</code>（parameter tuning）。通常使用验证集进行模型选择和调参，使用测试集评估模型的泛化能力。</p><h3 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h3><p>性能度量（performance measure），即为模型泛化能力的评价标准。给定数据集 D={(x⃗1,y1),(x⃗2,y2),⋯,(x⃗m,ym)}D = \left \{ (\vec x_1, y_1), (\vec x_2, y_2), \cdots, (\vec x_m, y_m) \right \}D={(x1​,y1​),(x2​,y2​),⋯,(xm​,ym​)}，其中 yiy_iyi​ 是样本 x⃗i\vec x_ixi​ 的真实标记。</p><p>回归任务常用的性能度量是<code>均方误差</code>（mean squared error）：</p><p>E(f;D)=∫x⃗∼D(f(x⃗)−y)2p(x⃗)dx⃗ E(f; \mathcal{D}) = \int_{\vec x \sim \mathcal D} (f(\vec x) - y)^2 p(\vec {x}) d\vec x E(f;D)=∫x∼D​(f(x)−y)2p(x)dx</p><p>分类任务常用的性能度量较多，常用的错误率：</p><p>E(f;D)=∫x⃗∼DI(f(x⃗)≠y)p(x⃗)dx⃗ E(f; \mathcal{D}) = \int_{\vec x \sim \mathcal D} \mathbb I(f(\vec x) \neq y) p(\vec {x}) d\vec x E(f;D)=∫x∼D​I(f(x)̸​=y)p(x)dx</p><p><code>准确率</code>（percision）和<code>召回率</code>（recall）：</p><p>P=TPTP+FPR=TPTP+FN \begin{aligned} P &amp;= \frac {TP} {TP + FP} \\ R &amp;= \frac {TP} {TP + FN} \end{aligned} PR​=TP+FPTP​=TP+FNTP​​</p><p>预测正例</p><p>预测负例</p><p>真实正例</p><p>TP</p><p>FN</p><p>真实负例</p><p>FP</p><p>TN</p><p>准确率和召回率不可得兼。以准确率作为纵轴、召回率作为横轴，可以得到<code>P-R曲线</code>，曲线中“准确率=召回率”的点成为<code>平衡点</code>（Break-Even Point）。</p><p>准确率和召回率的<code>调和平均</code>（harmonic mean）称为<code>F1</code>度量：</p><p>1F1=12(1P+1R)F1=2PRP+R \begin{aligned} \frac {1} {F1} &amp;= \frac {1} {2} (\frac {1} {P} + \frac {1} {R}) \\ F1 &amp;= \frac {2PR} {P + R} \end{aligned} F11​F1​=21​(P1​+R1​)=P+R2PR​​</p><p>由多组混淆矩阵计算多组准确率和召回率，再求平均值，可得<code>宏准确率</code>（macro-P）和<code>宏召回率</code>（macro-R）；将多组混淆矩阵求平均值，再求准确率和召回率，可得<code>微准确率</code>（micro-P）和<code>微召回率</code>（micro-R）。</p><p><code>ROC</code> 全称受试者工作特征（Receiver Operating Characteristic），该曲线以<code>真正例率</code>（True Positive Rate）为纵轴，以<code>假正例率</code>（False Positive Rate）为横轴：</p><p>TPR=TPTP+FNFPR=FPTN+FP \begin{aligned} TPR &amp;= \frac {TP} {TP + FN} \\ FPR &amp; = \frac {FP} {TN + FP} \end{aligned} TPRFPR​=TP+FNTP​=TN+FPFP​​</p><p>ROC 曲线下的面积称为<code>AUC</code>（Area Under ROC Curve），通常使用 AUC 作为ROC 曲线优劣的判断依据。</p><p>不同类型的错误所造成的后果不同，为权衡不同类型错误所造成的不同损失，可为错误赋予<code>非均等代价</code>（unequal cost）。令 D+D^+D+ 与 D−D^-D− 代表数据集 DDD 中的正例子集和反例子集，则<code>代价敏感</code>（cost-sensitive）错误率为：</p><p>E(f;D;cost)=1m(∑x⃗i∈D+I(f(x⃗i)≠yi)cost01+∑x⃗i∈D−I(f(x⃗i)≠yi)cost10) E(f; D; cost) = \frac {1} {m} \left ( \sum_{\vec x_i \in D^+} \mathbb I (f(\vec x_i) \neq y_i) cost_{01} + \sum_{\vec x_i \in D^-} \mathbb I (f(\vec x_i) \neq y_i) cost_{10} \right ) E(f;D;cost)=m1​⎝⎛​xi​∈D+∑​I(f(xi​)̸​=yi​)cost01​+xi​∈D−∑​I(f(xi​)̸​=yi​)cost10​⎠⎞​</p><h3 id="偏差与方差"><a href="#偏差与方差" class="headerlink" title="偏差与方差"></a>偏差与方差</h3><p><code>偏差-方差分解</code>（bias-veriance decomposition）是解释学习算法泛化性能的一种重要工具。对测试样本 x⃗\vec xx，令 yDy_DyD​ 为 x⃗\vec xx 在数据集中的标记，yyy 为 x⃗\vec xx 的真实标记，f(x⃗;D)f(\vec x; D)f(x;D) 为训练集 DDD 上学的模型 fff 在 x⃗\vec xx 上的预测输出。以回归任务为例，学习算法的期望预测为：</p><p>fˉ(x⃗)=ED[f(x⃗;D)] \bar f(\vec x) = \mathbb E_D \left [ f(\vec x; D) \right ] fˉ​(x)=ED​[f(x;D)]</p><p>期望输出与真实标记的差别称为<code>偏差</code>（bias）：</p><p>bias2(x⃗)=(fˉ(x⃗)−y)2 bias^2(\vec x) = \left ( \bar f(\vec x) - y \right )^2 bias2(x)=(fˉ​(x)−y)2</p><p>使用样本数相同的不同训练集产生的<code>方差</code>（variance）为：</p><p>var(x⃗)=ED[(f(x⃗;D)−fˉ(x⃗))2] var(\vec x) = \mathbb E_D \left [ \left (f(\vec x; D) - \bar f(\vec x) \right )^2 \right ] var(x)=ED​[(f(x;D)−fˉ​(x))2]</p><p>噪声为：</p><p>ε2=ED[(yD−y)2] \varepsilon ^2 = \mathbb E_D \left [ (y_D - y)^2 \right ] ε2=ED​[(yD​−y)2]</p><p>假定噪声的期望为零，可得：</p><p>E(f;D)=bias2(x⃗)+var(x⃗)+ε2 E(f; D) = bias^2(\vec x) + var(\vec x) + \varepsilon ^2 E(f;D)=bias2(x)+var(x)+ε2</p><p>即泛化误差可以分解为偏差、方差和噪声之和。偏差和方差间存在<code>偏差-方差窘境</code>（bias-variance dilemma），当学习算法训练不足时，学习器的拟合能力不够强，偏差主导了泛化错误率；当训练程度加深后，学习器的拟合能力足够，方差主导了泛化错误率。</p><h2 id="第-3-章-线性模型"><a href="#第-3-章-线性模型" class="headerlink" title="第 3 章 线性模型"></a>第 3 章 线性模型</h2><h3 id="基本形式"><a href="#基本形式" class="headerlink" title="基本形式"></a>基本形式</h3><p>给定由 ddd 个属性描述的示例 x=(x1;x2;⋯;xd)\boldsymbol x = (x_1; x_2; \cdots; x_d)x=(x1​;x2​;⋯;xd​)，其中 xix_ixi​ 是 x\boldsymbol xx 在第 iii 个属性上的取值，<code>线性模型</code>（linear model）试图学得一个通过属性的线性组合来进行预测的函数：</p><p>f(x)=wTx+b f(\boldsymbol x) = \boldsymbol w^T \boldsymbol x + b f(x)=wTx+b</p><p>其中 w=(w1;w2;⋯;wd)\boldsymbol w = (w_1; w_2; \cdots; w_d)w=(w1​;w2​;⋯;wd​)。</p><p>线性模型形式简单，易于建模，且 w\boldsymbol ww 直观表达了各属性在预测中的重要性，因此线性模型有很好的 <code>可解释性</code>（comprehensibility）。</p><p>在线性模型的基础上可通过引入层级结构或高维映射而得到更为强大的<code>非线性模型</code>（nonlinear model）。</p><h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><p>给定数据集 D={(x1,y1),(x2,y2),⋯,(xm,ym)}D = \{(\boldsymbol x_1, y_1), (\boldsymbol x_2, y_2), \cdots , (\boldsymbol x_m, y_m)\}D={(x1​,y1​),(x2​,y2​),⋯,(xm​,ym​)}，其中 xi=(xi1,xi2,⋯,xid)\boldsymbol x_i = (x_{i1}, x_{i2}, \cdots, x_{id})xi​=(xi1​,xi2​,⋯,xid​)，y∈Ry \in \mathbb Ry∈R，<code>线性回归</code>（linear regression）试图学得一个线性模型以尽可能准确地预测实际输出标记。</p><p>考虑最简单的单属性情形，D={(xi,yi)}i=1mD = \left \{ (x_i, y_i) \right \}_{i=1}^mD={(xi​,yi​)}i=1m​，线性回归试图学得</p><p>f(xi)=wxi+b f(x_i) = wx_i+b f(xi​)=wxi​+b</p><p>以使得 f(xi)≃yif(x_i) \simeq y_if(xi​)≃yi​。使用均方误差作为衡量 f(x)f(x)f(x) 与 yyy 之间差别的性能度量：</p><p>E(w,b)=∑i=1m(f(xi)−yi)2 E_{(w, b)} = \sum_{i=1}^{m} {\left (f(x_i) - y_i \right )^2} E(w,b)​=i=1∑m​(f(xi​)−yi​)2</p><p>则：</p><p>(w∗,b∗)=arg⁡min⁡(w,b)E(w,b) (w^*, b^*) = \underset {(w, b)} {\arg \min} E_{(w, b)} (w∗,b∗)=(w,b)argmin​E(w,b)​</p><p>均方误差有非常好的几何意义，它对应了 <code>欧氏距离</code>（Euclidean distance）。基于均方误差最小化来进行模型求解的方法称为 <code>最小二乘法</code>（least square method）。在线性回归中，最小二乘法试图找到一条直线，使得所有样本到直线上的欧氏距离之和最小。</p><p>E(w,b)E_{(w, b)}E(w,b)​ 是关于 www 和 bbb 的凸函数。对于区间 [a,b][a, b][a,b] 上定义的函数 fff，若它区间中任意两点 x1x_1x1​ 和 x2x_2x2​ 均有 f(x1+x22)≤f(x1)+f(x2)2f(\frac {x_1 + x_2} {2}) \le \frac {f(x_1) + f(x_2)} {2}f(2x1​+x2​​)≤2f(x1​)+f(x2​)​，则称 fff 为区间 [a,b][a, b][a,b] 上的凸函数。对实数集上的函数，可以通过求二阶导数的方式来判断，二阶导数在区间上非负则称为凸函数。</p><p>求解 www 和 bbb 使均方误差最小化的过程，称为线性回归模型的最小二乘 <code>参数估计</code>（parameter estimation）。将 E(w,b)E_{(w, b)}E(w,b)​ 分别对 www 和 bbb 求导，得到：</p><p>∂E(w,b)∂w=2(w∑i=1mxi2−∑i=1m(yi−b)xi)∂E(w,b)∂b=2(mb−∑i=1m(yi−wxi)) \begin {aligned} \frac {\partial E_{(w, b)}} {\partial w} &amp;= 2 \left ( w \sum_{i=1}^{m} x_i^2 - \sum_{i=1}^{m} (y_i - b)x_i \right ) \\ \frac {\partial E_{(w, b)}} {\partial b} &amp;= 2 \left ( mb - \sum_{i=1}^{m} (y_i - wx_i) \right ) \end {aligned} ∂w∂E(w,b)​​∂b∂E(w,b)​​​=2(wi=1∑m​xi2​−i=1∑m​(yi​−b)xi​)=2(mb−i=1∑m​(yi​−wxi​))​</p><p>对 www 和 bbb 的偏导置零可得到 www 和 bbb 最优解的 <code>闭式解</code>（closed-form solution）：</p><p>w=∑i=1mxiyi−mxˉyˉ∑i=1mxi2−mxˉ2b=yˉ−wxˉ \begin {aligned} w &amp;= \frac {\sum_{i=1}^{m}x_i y_i - m \bar x \bar y} {\sum_{i=1}^{m} {x_i^2} - m \bar x^2} \\ b &amp;= \bar y - w \bar x \end {aligned} wb​=∑i=1m​xi2​−mxˉ2∑i=1m​xi​yi​−mxˉyˉ​​=yˉ​−wxˉ​</p><p>更一般的情形，给定数据集 D={(xi,yi)}i=1mD = \left \{ (\boldsymbol x_i, y_i) \right \}_{i=1}^mD={(xi​,yi​)}i=1m​，其中 xi=(xi1,xi2,⋯,xid)\boldsymbol x_i = (x_{i1}, x_{i2}, \cdots, x_{id})xi​=(xi1​,xi2​,⋯,xid​)，y∈Ry \in \mathbb Ry∈R，线性回归试图学得：</p><p>f(xi)=wTxi+b f(\boldsymbol x_i) = \boldsymbol w^T \boldsymbol x_i+b f(xi​)=wTxi​+b</p><p>使得 f(xi)≃yif(\boldsymbol x_i) \simeq y_if(xi​)≃yi​。这称为 <code>多变量线性回归</code>（multivariate linear regression）。</p><p>[未完待续]</p><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ol><li>周志华. “机器学习.” 清华大学出版社，北京.</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;第-1-章-绪论&quot;&gt;&lt;a href=&quot;#第-1-章-绪论&quot; class=&quot;headerlink&quot; title=&quot;第 1 章 绪论&quot;&gt;&lt;/a&gt;第 1 章 绪论&lt;/h2&gt;&lt;h3 id=&quot;基本术语&quot;&gt;&lt;a href=&quot;#基本术语&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>大学课程</title>
    <link href="http://yixuxi.xyz/posts/yuque/%E5%A4%A7%E5%AD%A6%E8%AF%BE%E7%A8%8B/"/>
    <id>http://yixuxi.xyz/posts/yuque/大学课程/</id>
    <published>2019-03-26T02:54:36.000Z</published>
    <updated>2019-11-04T02:00:30.412Z</updated>
    
    <content type="html"><![CDATA[<p><a name="5563b44c"></a></p><h2 id="北京大学"><a href="#北京大学" class="headerlink" title="北京大学"></a>北京大学</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a name=&quot;5563b44c&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;北京大学&quot;&gt;&lt;a href=&quot;#北京大学&quot; class=&quot;headerlink&quot; title=&quot;北京大学&quot;&gt;&lt;/a&gt;北京大学&lt;/h2&gt;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>事实上</title>
    <link href="http://yixuxi.xyz/posts/yuque/%E4%BA%8B%E5%AE%9E%E4%B8%8A/"/>
    <id>http://yixuxi.xyz/posts/yuque/事实上/</id>
    <published>2019-03-07T00:08:49.000Z</published>
    <updated>2019-11-04T02:00:30.404Z</updated>
    
    <content type="html"><![CDATA[<p>事实上搜索搜索</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;事实上搜索搜索&lt;/p&gt;

      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>如何科学减肥</title>
    <link href="http://yixuxi.xyz/posts/yuque/%E5%A6%82%E4%BD%95%E7%A7%91%E5%AD%A6%E5%87%8F%E8%82%A5/"/>
    <id>http://yixuxi.xyz/posts/yuque/如何科学减肥/</id>
    <published>2019-01-31T01:50:22.000Z</published>
    <updated>2019-11-04T02:00:30.416Z</updated>
    
    <content type="html"><![CDATA[<p><a name="9699a50e"></a></p><h3 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h3><ul><li><p>书单</p></li><li><p>app</p></li><li><p>营养学</p></li><li><p>低碳饮食</p></li><li><p>生酮饮食</p></li><li><p>paper</p></li><li><p>阿特金斯减肥法</p></li><li><p>计划</p></li><li><p>表格</p></li><li><p>体脂称</p></li><li><p>手环<br><a name="d41d8cd9"></a></p><h3 id=""><a href="#" class="headerlink" title=""></a></h3><p><a name="b59c9e0f"></a></p><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3></li><li><p>基础代谢</p></li><li><p>低碳水饮食</p></li><li><p>燃脂心率</p></li><li><p>BMI 指数</p></li><li><p>生酮饮食(keto)</p></li><li><p>HIIT</p><ul><li>Tabata<blockquote><p>Tabata 是 HIIT 高强度间歇性的一种，它的训练方式和 HIIT 有些区别，但减脂原理以及优点和 HIIT 都是类似的。</p></blockquote></li></ul></li></ul><p><a name="fef9d212"></a></p><h2 id="肥胖的危害"><a href="#肥胖的危害" class="headerlink" title="肥胖的危害"></a>肥胖的危害</h2><p><img src="https://cdn.nlark.com/yuque/0/2019/jpeg/223800/1549541591417-d6dd7785-cdbb-4f09-b3ae-58d4f5b1bf0c.jpeg#align=left&display=inline&height=386&originHeight=328&originWidth=550&size=0&width=648" alt=""><br><a name="e9d1af34"></a></p><h2 id="减肥方法"><a href="#减肥方法" class="headerlink" title="减肥方法"></a>减肥方法</h2><p><a name="44ebfb82"></a></p><h3 id="饮食"><a href="#饮食" class="headerlink" title="饮食"></a>饮食</h3><p><a name="445ddd1a"></a></p><h4 id="生酮饮食"><a href="#生酮饮食" class="headerlink" title="生酮饮食"></a>生酮饮食</h4><ul><li><a href="http://songshuhui.net/archives/101982" target="_blank" rel="noopener">治疗癫痫病的生酮饮食，你想用来减肥？</a><blockquote><p>生酮饮食是限制碳水化合物、大量摄入脂肪的食谱，典型的生酮食谱中脂肪供能比高达70%-80%，蛋白质约为10%-20%，而碳水化合物只占5%-10%<br>人体的生理活动需要热量，正常情况下由碳水化合物来提供。光是大脑活动所需要的热量，每天就需要120克葡萄糖。当人体处于饥饿或者碳水化合物缺乏的状态，就会消耗肝糖元以及暂时分解肌肉来供能。这样的状态持续三四天，糖元消失殆尽，血液中的胰岛素水平大大降低，肝脏就会开始把脂肪分解为酮体来供能。<br>身体本来是按照葡萄糖供能来运行的，在生酮饮食下改用酮体来供能。就像一台汽油发动机，变成了用酒精做燃料。这种改变，会改变身体的整个代谢状态。</p></blockquote></li></ul><p><strong>优点</strong>：</p><ul><li>短期的生酮饮食对于减轻体重以及改善某些生理指标有一定帮助<ul><li>体重的降低</li><li>体脂率、胰岛素水平、血压、腰臀比等指标也有所改善 </li></ul></li></ul><p><strong>缺点：</strong></p><blockquote><p>当血液中的酮体含量过高，人体会出于酮血症的状态。在这种状态下，肾会排出酮体和体液，从而导致人体失水，体重迅速减轻。或许，这就是有的人采用生酮饮食能迅速减重的原因。</p></blockquote><ul><li><p>其他健康的减肥方式相比，生酮饮食就没有什么优势</p></li><li><p>生酮饮食的营养组成相当不合理。</p><blockquote><p>许多维生素、矿物质以及膳食纤维等营养成分都伴随着碳水化合物而来。长期坚持生酮饮食，营养的失衡对健康会有什么样的影响，也还没有充分的研究</p></blockquote></li><li><p>人体可能会出现许多不适，比如饥饿、疲劳、情绪低落、便秘、头痛等等</p></li><li><p>较长时间的生酮饮食会大大增加肾结石、骨质疏松和高尿酸的风险</p></li></ul><p>总结:</p><blockquote><p>从实验结果来看，短期的生酮饮食对于减轻体重以及改善某些生理指标有一定帮助。不过，如果跟其他健康的减肥方式相比，生酮饮食就没有什么优势。而生酮饮食早已明确的副作用，就已经不可忽视。此外，生酮饮食的营养组成相当不合理。许多维生素、矿物质以及膳食纤维等营养成分都伴随着碳水化合物而来。长期坚持生酮饮食，营养的失衡对健康会有什么样的影响，也还没有充分的研究。<br>当然，如果作为一种医疗手段，生酮饮食还是值得研究和关注的。在某些疾病的治疗中，经过医生的评估，并且在医院的密切监护下进行生酮饮食，也是可以采取的方案。至于日常生活中用它来减肥，还是算了吧。</p></blockquote><p>简单来说<strong>生酮饮食</strong>最初就是作为一种医疗手段，例如糖尿病、癫痫，生酮饮食还被用于许多跟神经有关的疾病上，比如阿茨海默症、自闭症、脑癌、帕金森氏病等等，有一定的效果，不过综合权衡效果与证据强度，并没有达到临床推荐的层次。而且副作用明显，便秘和肾结石<br><a name="d41d8cd9"></a></p><h4 id="-1"><a href="#-1" class="headerlink" title=""></a></h4><p><a name="0d74ba8c"></a></p><h4 id="阿特金斯减肥法"><a href="#阿特金斯减肥法" class="headerlink" title="阿特金斯减肥法"></a>阿特金斯减肥法</h4><p><a name="6b17f352"></a></p><h4 id="低碳水饮食"><a href="#低碳水饮食" class="headerlink" title="低碳水饮食"></a>低碳水饮食</h4><p><a name="37b6debe"></a></p><h3 id="运动"><a href="#运动" class="headerlink" title="运动"></a>运动</h3><p><a name="8558e728"></a></p><h4 id="长时间持续性有氧"><a href="#长时间持续性有氧" class="headerlink" title="长时间持续性有氧"></a>长时间持续性有氧</h4><blockquote><p>运动强度超过30分钟的中低强度有氧，如慢跑，游泳，长途骑行等。是一直以来没毛病的传统型有氧。</p></blockquote><p>不低于30分钟不高于70分钟为最佳。</p><ul><li>慢跑</li><li>游泳</li><li>长途骑行</li></ul><p><img src="https://cdn.nlark.com/yuque/0/2019/png/223800/1549541856051-0fd9ec25-c919-4b06-bd75-e991e3564695.png#align=left&display=inline&height=707&name=image.png&originHeight=508&originWidth=440&size=300058&width=612" alt="image.png"><br />卡氏公式:<br />        燃脂心率 =（220-年龄-静态心率）x 30%-45%+静态心率</p><p><br /><a name="95a7e4d0"></a></p><h4 id="间歇性有氧运动（HIIT）"><a href="#间歇性有氧运动（HIIT）" class="headerlink" title="间歇性有氧运动（HIIT）"></a>间歇性有氧运动（HIIT）</h4><blockquote><p>HIIT，简单讲就是一种高强度与低强度交替间歇的训练方式。只要在运动中是强度高低交替的，都可以视作广义的HIIT，比如快慢交替跑，快慢交替骑车等等。</p></blockquote><p>优点：</p><ul><li>即运动后仍然能保持高热量、高脂肪的消耗效率。其训练后减脂效果可以持续72小时左右。并且其高强度的动作还可以起到保护肌肉维持代谢的作用</li></ul><p>缺点：</p><ul><li><p>强度高<br><a name="d41d8cd9"></a></p><h3 id="-2"><a href="#-2" class="headerlink" title=""></a></h3><p><a name="0debf521"></a></p><h3 id="计划"><a href="#计划" class="headerlink" title="计划"></a>计划</h3></li><li><p>食谱</p></li><li><p>运动</p><ul><li>慢跑<blockquote><p>跑完步记得补充水份，牛奶比矿泉水更补水，还能补充蛋白质和电解质修复你的身体损耗。<br>吃一根富含钾的香蕉也不错。</p></blockquote></li></ul></li></ul><p><a name="1c7fa641"></a></p><h4 id="注意事项："><a href="#注意事项：" class="headerlink" title="注意事项："></a>注意事项：</h4><ul><li><p>减肥也不能吃得低于自己的基础代谢<br><a name="5b208a57"></a></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考:"></a>参考:</h2></li><li><p>知乎</p><ul><li><a href="https://www.zhihu.com/question/20234124/answer/352356794" target="_blank" rel="noopener">禁食「碳水化合物」正确吗？「阿特金斯减肥法」有效安全吗？ - 《科学世界》杂志的回答 - 知乎 </a></li><li><a href="https://www.zhihu.com/question/23586456/answer/151934876" target="_blank" rel="noopener">哥本哈根食谱真的有那么神奇吗？</a></li><li><a href="https://zhuanlan.zhihu.com/p/36154483" target="_blank" rel="noopener">减肥谣言大混战|低碳水or低脂肪，谁在瞎说|生酮减肥</a></li><li><a href="https://zhuanlan.zhihu.com/p/36020258" target="_blank" rel="noopener">脂肪是如何被消耗的？</a></li><li><a href="https://www.zhihu.com/question/28248712" target="_blank" rel="noopener">tabata 4分钟真的有效果吗？</a></li></ul></li><li><p>丁香园</p><ul><li><a href="http://endo.dxy.cn/article/560367" target="_blank" rel="noopener">(丁香园)生酮饮食：减重效果如何？又有什么风险？</a><a href="https://web.archive.org/web/20190207075150/http://endo.dxy.cn/article/560367" target="_blank" rel="noopener">备用链接</a> #生酮饮食</li></ul></li><li><p>RubyChina</p><ul><li><a href="https://ruby-china.org/topics/38029" target="_blank" rel="noopener">推荐一个简单有效的减肥方法：生酮饮食</a> <a href="https://web.archive.org/web/20190207074707/https://ruby-china.org/topics/38029" target="_blank" rel="noopener">备用链接</a></li></ul></li><li><p>Reddit</p><ul><li><a href="https://www.reddit.com/r/keto/wiki/faq" target="_blank" rel="noopener">keto faq</a></li><li><a href="https://calculo.io/keto-calculator" target="_blank" rel="noopener">keto-calculator</a></li><li><a href="https://www.reddit.com/r/keto/wiki/keto_in_a_nutshell" target="_blank" rel="noopener">Keto For Beginners</a></li><li><a href="https://docs.google.com/spreadsheets/d/1g-tl1PqIUaHsLdL2Au1KKGe7z5EQS0_xT7f_lMllIB4/edit#gid=5" target="_blank" rel="noopener">Keto Progress Tracker</a></li></ul></li><li><p>V2EX</p><ul><li><a href="https://www.v2ex.com/t/469642" target="_blank" rel="noopener">一个女肥宅的减肥&amp;健身心得，和大家分享~</a></li></ul></li><li><p>科学松鼠会</p><ul><li><a href="http://songshuhui.net/archives/101982" target="_blank" rel="noopener">治疗癫痫病的生酮饮食，你想用来减肥？</a></li><li><a href="http://songshuhui.net/archives/100583" target="_blank" rel="noopener">低脂肪和低碳水，哪种食谱减肥效果好？</a></li></ul></li><li><p>其它</p><ul><li><a href="http://josepharcita.blogspot.com/2011/03/guide-to-ketosis.html" target="_blank" rel="noopener">A Guide to Ketosis</a><br><a name="app"></a><h4 id="app"><a href="#app" class="headerlink" title="app"></a>app</h4></li></ul></li><li><p>keep</p></li></ul><p><a name="b6c5522e"></a></p><h4 id="书单"><a href="#书单" class="headerlink" title="书单"></a>书单</h4><p>source: <a href="https://www.douban.com/doulist/111781954/" target="_blank" rel="noopener">https://www.douban.com/doulist/111781954/</a></p><ul><li>《我们为什么会发胖》</li></ul><p><a href="https://book.douban.com/subject/26369484/" target="_blank" rel="noopener"><img src="https://cdn.nlark.com/yuque/0/2019/png/223800/1549531654316-1583207e-f17f-4b6c-af24-ebb7d5e1a4a9.png#align=left&display=inline&height=295&name=image.png&originHeight=602&originWidth=1480&size=247921&width=726" alt="image.png"></a></p><ul><li>《运动饮食1:9》</li></ul><p><img src="https://cdn.nlark.com/yuque/0/2019/png/223800/1549531774483-242f4e73-f064-4451-924d-40307caf186a.png#align=left&display=inline&height=308&name=image.png&originHeight=616&originWidth=1500&size=216699&width=750" alt="image.png"></p><ul><li>《施瓦辛格健身全书》</li></ul><p><img src="https://cdn.nlark.com/yuque/0/2019/png/223800/1549531883167-4ed718e6-8487-494d-b7ed-46a7809bbd9e.png#align=left&display=inline&height=267&name=image.png&originHeight=534&originWidth=1496&size=322007&width=748" alt="image.png"></p><ul><li>《囚徒健身》</li></ul><p><img src="https://cdn.nlark.com/yuque/0/2019/png/223800/1549532264981-490bb0e2-e58d-4f9e-b91b-445fc496b0a4.png#align=left&display=inline&height=307&name=image.png&originHeight=614&originWidth=1468&size=363357&width=734" alt="image.png"></p><ul><li>《核心基础运动》</li></ul><p><img src="https://cdn.nlark.com/yuque/0/2019/png/223800/1549532305042-6752e6dd-d89b-4727-992d-856408a3ca2d.png#align=left&display=inline&height=331&name=image.png&originHeight=662&originWidth=1514&size=312834&width=757" alt="image.png"></p><ul><li>《德拉威尔拉伸训练图解》</li></ul><p><img src="https://cdn.nlark.com/yuque/0/2019/png/223800/1549532339425-0c31d2a0-5da1-4e2a-81c5-1cb7a0312ac6.png#align=left&display=inline&height=304&name=image.png&originHeight=608&originWidth=1472&size=275937&width=736" alt="image.png"><br /></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a name=&quot;9699a50e&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;关键词&quot;&gt;&lt;a href=&quot;#关键词&quot; class=&quot;headerlink&quot; title=&quot;关键词&quot;&gt;&lt;/a&gt;关键词&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;书单&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;app&lt;/p
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>无标题</title>
    <link href="http://yixuxi.xyz/posts/yuque/%E6%97%A0%E6%A0%87%E9%A2%98/"/>
    <id>http://yixuxi.xyz/posts/yuque/无标题/</id>
    <published>2019-01-04T00:04:35.000Z</published>
    <updated>2019-11-04T02:00:30.408Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Deep Learning重要发展脉络</title>
    <link href="http://yixuxi.xyz/posts/yuque/Deep%20Learning%E9%87%8D%E8%A6%81%E5%8F%91%E5%B1%95%E8%84%89%E7%BB%9C/"/>
    <id>http://yixuxi.xyz/posts/yuque/Deep Learning重要发展脉络/</id>
    <published>2018-12-26T06:32:12.000Z</published>
    <updated>2019-11-04T02:00:30.408Z</updated>
    
    <content type="html"><![CDATA[<p>Deep Learning（深度学习）的概念源于人工神经网络的研究，它的概念由Hinton等人于2006年提出，但它的模型经历了怎样的发展和演化，本文将为您深度解读Deep Learning的前世今生。<br /><img src="https://static.aminer.cn/rcd/article/expertpic/aminer.gif#align=left&display=inline&height=1205&originHeight=1205&originWidth=1694&search=&status=done&width=1694" alt=""><br /><br><a name="tfacwu"></a></p><h3 id="脉络一-cv-tensor"><a href="#脉络一-cv-tensor" class="headerlink" title="脉络一  cv/tensor"></a>脉络一  cv/tensor</h3><p>1943年  心理学家麦卡洛可（W·McCulloch）和数理逻辑学家皮茨（W·Pitts）参考了生物神经元的结构，发表了《神经活动中思想内在性的逻辑运算》一文，提出了抽象的神经元模型MP，该模型可以看做深度学习的雏形。</p><p>1957年  认知心理学大师 Frank Rosenblatt 发明了感知机（Perceptron，又称感知器），感知机是当时首个可以学习的人工神经网络，掀起了一股学习的热潮。</p><p>1969年  人工智能大师 Marvin Minksy 和 Seymour Papert 在《Perceptron》一书中，用详细的数学证明了感知机的弱点，没有隐层的简单感知机在许多像XOR问题的情形下显得无能为力，并证明了简单感知机只能解决线性分类问题和一阶谓诃同题。神经网络研究进入冰河期。</p><p>1984年  日本学者福岛邦彦（Kunihiko Fukishima）提出了卷积神经网络的原始模型神经感知机（Neocognitron），产生了卷积和池化的思想（当时不叫卷积和池化）。</p><p>1986年  Hinton等人正式提出一般 Delta 法则，即反向传播（BP）算法，并用反向传播训练MLP（多层感知机）。但其实在他提出之前，已经有人将其付诸实际。（1985年 Parter 也独立地得出过相似的算法，他称之为学习逻辑。此外，1985年 Lecun 也研究出大致相似的学习法则。）</p><p>1998年  以 Yann LeCun 为首的研究人员实现了一个5层的卷积神经网络——LeNet-5，以识别手写数字。LeNet-5 标志着 CNN（卷积神经网络）的真正面世，LeNet-5 的提出把 CNN 推上了一个小高潮。</p><p>之后SVM（支持向量机）兴起，SVM在计算及准确度上都有较大的优势，导致卷积神经网络的方法在后来的一段时间并未能火起来。</p><p>2012年  Hinton组的 AlexNet 在 ImageNet 上以巨大优势夺冠，掀起了深度学习的热潮。AlexNet 可以算是 LeNet 的一种更深更宽的版本，并加上了 relu、dropout 等技巧。</p><p>这条思路被后人发展，出现了 VGG，GoogLeNet 等网络。</p><p>2016年  青年计算机视觉科学家何恺明在层次之间加入了跳跃连接，Resnet 极大增加了网络深度，效果有很大提升。另一个将这条思路继续发展下去的是去年cvpr best paper densenet。</p><p>除此之外，cv领域的特定任务还出现了各种各样的模型（Mask-RCNN等），这里不一一介绍。</p><p>2017年  Hinton认为反省传播和传统神经网络有缺陷，继而提出了 Capsule Net。但是目前在 cifar 等数据集上效果一般，这个思路还需要继续验证和发展。</p><p><a name="orhvmd"></a></p><h3 id="脉络二-生成模型"><a href="#脉络二-生成模型" class="headerlink" title="脉络二  生成模型"></a>脉络二  生成模型</h3><p>传统的生成模型是要预测联合概率分布P(x,y)。</p><p>RBM（受限玻尔兹曼机）这个模型其实是一个基于能量的模型，1986年的时候就有，2006年将其重新拿出来作为一个生成模型，并且堆叠成为deep belief network，使用逐层贪婪或者wake-sleep的方法训练，不过这个模型效果一般，现在已经没什么人提了。但是Hinton等人却从此开始使用深度学习重新包装神经网络。</p><p>Auto-Encoder是上个世纪80年代hinton提出的模型，如今由于计算能力的进步重新登上舞台。2008年，Bengio等人又搞了denoise Auto-Encoder。</p><p>Max welling等人使用神经网络训练一个有一层隐变量的图模型，由于使用了变分推断，最后长得跟auto-encoder有点像，因而被称为Variational auto-encoder。此模型可以通过隐变量的分布采样，经过后面的decoder网络直接生成样本。</p><p>GAN（生成对抗网络）是于2014年提出的模型，如今炙手可热。它是一个生成模型，通过判别器D（Discriminator）和生成器G（Generator）的对抗训练，直接使用神经网络G隐式建模样本整体的概率分布。每次运行便相当于从分布中采样。</p><p>DCGAN是一个相当好的卷积神经网络实现，而WGAN则是通过维尔斯特拉斯距离替换原来的JS散度来度量分布之间的相似性的工作，使得训练稳定。PGGAN则逐层增大网络，生成机器逼真的人脸。</p><p><a name="rkublv"></a></p><h3 id="脉络三-sequencelearning"><a href="#脉络三-sequencelearning" class="headerlink" title="脉络三 sequencelearning"></a>脉络三 sequencelearning</h3><p>1982年  出现的hopfield network有了递归网络的思想。</p><p>1997年  Jürgen Schmidhuber发明LSTM，并做了一系列的工作。但是更有影响力的还是2013年由Hinton组使用RNN做的语音识别工作，这种方法比传统方法更强。</p><p>文本方面，Bengio在svm最火的时期提出了一种基于神经网络的语言模型，后来Google提出的word2vec也包含了一些反向传播的思想。在机器翻译等任务上，逐渐出现了以RNN为基础的seq2seq模型（序列模型），模型通过一个encoder（编码器）把一句话的语义信息压成向量再通过decoder（解码器）输出，当然更多的还要和Attention Model（注意力模型）结合。</p><p>后来，大家发现使用以字符为单位的CNN模型在很多语言任务也有不俗的表现，而且时空消耗更少。LSTM/RNN 模型中的Attention机制是用于克服传统编码器-解码器结构存在的问题的。其中，self-attention（自注意力机制）实际上就是采取一种结构去同时考虑同一序列局部和全局的信息，Google就有一篇耸人听闻的Attention Is All You Need的文章。</p><p><a name="65uhmf"></a></p><h3 id="脉络四-deepreinforcement-learning"><a href="#脉络四-deepreinforcement-learning" class="headerlink" title="脉络四 deepreinforcement learning"></a>脉络四 deepreinforcement learning</h3><p>该领域最出名的是DeepMind，这里列出的David Silver则是一直研究rl（强化学习）的高管。</p><p>q-learning是很有名的传统rl算法，deep q-learning则是将原来的q值表用神经网络代替。利用deep q-learning制作的打砖块的游戏十分有名。后来David Silver等人又利用其测试了许多游戏，发在了Nature上。</p><p>增强学习在double duel的进展，主要是Qlearning的权重更新时序上。</p><p>DeepMind的其他工作诸如DDPG、A3C也非常有名，它们是基于policy gradient和神经网络结合的变种。</p><p>大家都知道的一个应用是AlphaGo，里面不仅使用了rl的方法，也包含了传统的蒙特卡洛搜索技巧。Alpha Zero 则是他们搞了一个用Alphago框架来打其他棋类游戏的游戏，而且这个“打”还是吊打的打。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Deep Learning（深度学习）的概念源于人工神经网络的研究，它的概念由Hinton等人于2006年提出，但它的模型经历了怎样的发展和演化，本文将为您深度解读Deep Learning的前世今生。&lt;br /&gt;&lt;img src=&quot;https://static.amine
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Elasticsearch 知识点索引</title>
    <link href="http://yixuxi.xyz/posts/yuque/Elasticsearch%20%E7%9F%A5%E8%AF%86%E7%82%B9%E7%B4%A2%E5%BC%95/"/>
    <id>http://yixuxi.xyz/posts/yuque/Elasticsearch 知识点索引/</id>
    <published>2018-12-17T01:28:38.000Z</published>
    <updated>2019-11-04T02:00:30.408Z</updated>
    
    <content type="html"><![CDATA[<p><a name="9ykeeh"></a></p><h1 id="quick-start"><a href="#quick-start" class="headerlink" title="quick start"></a>quick start</h1><p><a name="qrpeom"></a></p><h2 id="install"><a href="#install" class="headerlink" title="install"></a>install</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a name=&quot;9ykeeh&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;quick-start&quot;&gt;&lt;a href=&quot;#quick-start&quot; class=&quot;headerlink&quot; title=&quot;quick start&quot;&gt;&lt;/a&gt;quick start&lt;/h1&gt;&lt;p&gt;&lt;a na
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>测试图片</title>
    <link href="http://yixuxi.xyz/posts/yuque/%E6%B5%8B%E8%AF%95%E5%9B%BE%E7%89%87/"/>
    <id>http://yixuxi.xyz/posts/yuque/测试图片/</id>
    <published>2018-12-12T22:22:01.000Z</published>
    <updated>2019-11-04T02:00:30.412Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.nlark.com/yuque/0/2019/png/223800/1572711795535-706c0409-f726-4627-9d0a-e3d799d7fd00.png#align=left&display=inline&height=225&name=images%20%283%29.png&originHeight=225&originWidth=225&search=&size=1636&status=done&width=225" alt="images (3).png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://cdn.nlark.com/yuque/0/2019/png/223800/1572711795535-706c0409-f726-4627-9d0a-e3d799d7fd00.png#align=left&amp;display=inline&amp;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>测试文档</title>
    <link href="http://yixuxi.xyz/posts/yuque/%E6%B5%8B%E8%AF%95%E6%96%87%E6%A1%A3/"/>
    <id>http://yixuxi.xyz/posts/yuque/测试文档/</id>
    <published>2018-12-11T22:43:12.000Z</published>
    <updated>2019-11-04T02:00:30.412Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yixuxi.xyz/posts/hello-world/"/>
    <id>http://yixuxi.xyz/posts/hello-world/</id>
    <published>2016-04-05T06:16:00.000Z</published>
    <updated>2019-11-04T01:59:48.923Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
      <category term="math" scheme="http://yixuxi.xyz/categories/math/"/>
    
    
  </entry>
  
</feed>

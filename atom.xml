<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>å™«åæˆ</title>
  
  <subtitle>Just Do IT.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yixuxi.xyz/"/>
  <updated>2019-11-14T08:40:04.315Z</updated>
  <id>http://yixuxi.xyz/</id>
  
  <author>
    <name>zhiyue</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Regularization on GBDT</title>
    <link href="http://yixuxi.xyz/posts/hello-world_2/"/>
    <id>http://yixuxi.xyz/posts/hello-world_2/</id>
    <published>2019-11-14T08:40:04.315Z</published>
    <updated>2019-11-14T08:40:04.315Z</updated>
    
    <content type="html"><![CDATA[<script src="//gist.github.com/930f5f2f2b315ec967617d88600f16df.js"></script><p>ä¹‹å‰ä¸€ç¯‡æ–‡ç« ç®€å•åœ°è®²äº†XGBoostçš„å®ç°ä¸æ™®é€šGBDTå®ç°çš„ä¸åŒä¹‹å¤„ï¼Œæœ¬æ–‡å°è¯•æ€»ç»“ä¸€ä¸‹GBDTè¿ç”¨çš„æ­£åˆ™åŒ–æŠ€å·§ã€‚</p><script src="https://gist.github.com/zihengcat/1a80a31b671e5bb3db6eb7af5a4b30f7.js"></script><h3 id="Early-Stopping"><a href="#Early-Stopping" class="headerlink" title="Early Stopping"></a>Early Stopping</h3><p><a href="https://en.wikipedia.org/wiki/Early_stopping" target="_blank" rel="noopener">Early Stopping</a>æ˜¯æœºå™¨å­¦ä¹ è¿­ä»£å¼è®­ç»ƒæ¨¡å‹ä¸­å¾ˆå¸¸è§çš„é˜²æ­¢è¿‡æ‹ŸåˆæŠ€å·§ï¼Œç»´åŸºç™¾ç§‘é‡Œå¦‚ä¸‹æè¿°:</p><blockquote><p>In machine learning, early stopping is a form of <em>regularization</em> used to <em>avoid overfitting</em> when training a learner with an <em>iterative method</em>, such as gradient descent.</p></blockquote><p>å…·ä½“çš„åšæ³•æ˜¯é€‰æ‹©ä¸€éƒ¨åˆ†æ ·æœ¬ä½œä¸ºéªŒè¯é›†ï¼Œåœ¨è¿­ä»£æ‹Ÿåˆè®­ç»ƒé›†çš„è¿‡ç¨‹ä¸­ï¼Œå¦‚æœæ¨¡å‹åœ¨éªŒè¯é›†é‡Œé”™è¯¯ç‡ä¸å†ä¸‹é™ï¼Œå°±åœæ­¢è®­ç»ƒï¼Œä¹Ÿå°±æ˜¯è¯´æ§åˆ¶è¿­ä»£çš„è½®æ•°ï¼ˆæ ‘çš„ä¸ªæ•°ï¼‰ã€‚</p><p>XGBoost Pythonå…³äºearly stoppingçš„<a href="https://github.com/dmlc/xgboost/blob/master/doc/python/python_intro.md#early-stopping" target="_blank" rel="noopener">å‚æ•°è®¾ç½®æ–‡æ¡£</a>éå¸¸æ¸…æ™°ï¼ŒAPIå¦‚ä¸‹ï¼š</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># code snippets from xgboost python-package training.py</span><br><span class="line">def train(..., evals=(), early_stopping_rounds=None)</span><br><span class="line">&quot;&quot;&quot;Train a booster with given parameters.</span><br><span class="line">Parameters</span><br><span class="line">    ----------</span><br><span class="line">early_stopping_rounds: int</span><br><span class="line">        Activates early stopping. Validation error needs to decrease at least</span><br><span class="line">        every &lt;early_stopping_rounds&gt; round(s) to continue training.</span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure><p>Sklearnçš„GBDTå®ç°è™½ç„¶å¯ä»¥æ·»åŠ early stoppingï¼Œä½†æ˜¯æ¯”è¾ƒå¤æ‚ã€‚å®˜æ–¹æ²¡æœ‰ç›¸åº”çš„æ–‡æ¡£å’Œä»£ç æ ·ä¾‹ï¼Œå¿…é¡»çœ‹<a href="https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/gradient_boosting.py#L931" target="_blank" rel="noopener">æºç </a>ã€‚å®ç°çš„æ—¶å€™éœ€è¦ç”¨æˆ·æä¾›monitorå›è°ƒå‡½æ•°ï¼Œä¸”è¦äº†è§£æºç å†…éƒ¨_fit_stageså‡½æ•°çš„localsï¼Œæ€»ä¹‹å¯¹æ–°æ‰‹å¾ˆä¸å‹å¥½ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#code snippets from sklearn.ensemble.gradient_boosting</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaseGradientBoosting</span><span class="params">(six.with_metaclass<span class="params">(ABCMeta, BaseEnsemble,</span></span></span></span><br><span class="line"><span class="class"><span class="params"><span class="params">   _LearntSelectorMixin)</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Abstract base class for Gradient Boosting. """</span></span><br><span class="line">...</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y, sample_weight=None, monitor=None)</span>:</span></span><br><span class="line"><span class="string">"""Fit the gradient boosting model.</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">----------</span></span><br><span class="line"><span class="string">monitor : callable, optional</span></span><br><span class="line"><span class="string">            The monitor is called after each iteration with the current</span></span><br><span class="line"><span class="string">            iteration, a reference to the estimator and the local variables of</span></span><br><span class="line"><span class="string">            ``_fit_stages`` as keyword arguments ``callable(i, self,</span></span><br><span class="line"><span class="string">            locals())``. If the callable returns ``True`` the fitting procedure</span></span><br><span class="line"><span class="string">            is stopped. The monitor can be used for various things such as</span></span><br><span class="line"><span class="string">            computing held-out estimates, early stopping, model introspect, and</span></span><br><span class="line"><span class="string">            snapshoting.</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><p>å¯¹Sklearnæ„Ÿå…´è¶£çš„å¯ä»¥çœ‹è¿™ç¯‡æ–‡ç« <a href="*https://henri.io/posts/using-gradient-boosting-with-early-stopping.html">Using Gradient Boosting (with Early Stopping)</a>ï¼Œé‡Œé¢æœ‰å›è°ƒå‡½æ•°monitorçš„å‚è€ƒå®ç°ã€‚</p><h3 id="Shrinkage"><a href="#Shrinkage" class="headerlink" title="Shrinkage"></a>Shrinkage</h3><p>Shrinkageå°±æ˜¯å°†æ¯æ£µæ ‘çš„è¾“å‡ºç»“æœä¹˜ä¸€ä¸ªå› å­($0&lt;\nu&lt;1$)ï¼Œå…¶ä¸­$\Sigma_{j=1}^{J_m}\gamma_{jm}I(x\in R_{jm})$æ˜¯ç¬¬mæ£µçš„è¾“å‡ºï¼Œè€Œ$f(m)$æ˜¯å‰mæ£µæ ‘çš„ensemble: $$f_m(x) = f_{m-1}(x) + \nu\cdot\Sigma_{j=1}^{J_m}\gamma_{jm}I(x\in R_{jm})$$<br>ESLä¹¦ä¸­è¿™æ ·è®²ï¼š</p><blockquote><p>The parameter $\nu$ can be regarded as controlling the leanring rate of the boosting procedure</p></blockquote><p>$\nu$å’Œè¿­ä»£è½®æ•°M(æ ‘ä¸ªæ•°)æ˜¯ä¸€ä¸ªtradeoffï¼Œæ¨èçš„æ˜¯$\nu$å€¼è®¾ç½®å°ä¸€ç‚¹(å¦‚0.1)ï¼Œè€ŒMè®¾ç½®å¤§ä¸€äº›ã€‚è¿™æ ·ä¸€èˆ¬èƒ½æœ‰æ¯”è¾ƒå¥½çš„å‡†ç¡®ç‡ï¼Œä»£ä»·æ˜¯è®­ç»ƒæ—¶é—´å˜é•¿(ä¸Mæˆæ¯”ä¾‹)ã€‚</p><p>ä¸‹é¢æ˜¯Sklearnçš„å®ç°å…³äºè¯¥å‚æ•°è®¾ç½®çš„ç‰‡æ®µï¼ŒXGBoostç±»ä¼¼ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#code snippets from sklearn.ensemble.gradient_boosting</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GradientBoostingClassifier</span><span class="params">(BaseGradientBoosting, ClassifierMixin)</span>:</span></span><br><span class="line">    <span class="string">"""Gradient Boosting for classification."""</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ..., learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">100</span>, ...)</span>:</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">learning_rate : float, optional (default=0.1)</span></span><br><span class="line"><span class="string">        learning rate shrinks the contribution of each tree by `learning_rate`.</span></span><br><span class="line"><span class="string">        There is a trade-off between learning_rate and n_estimators.</span></span><br><span class="line"><span class="string">n_estimators : int (default=100)</span></span><br><span class="line"><span class="string">        The number of boosting stages to perform. Gradient boosting</span></span><br><span class="line"><span class="string">        is fairly robust to over-fitting so a large number usually</span></span><br><span class="line"><span class="string">        results in better performance</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><h3 id="Subsampling"><a href="#Subsampling" class="headerlink" title="Subsampling"></a>Subsampling</h3><p>Subsamplingå…¶å®æºäºbootstrap averaging(bagging)æ€æƒ³ï¼ŒGBDTé‡Œçš„åšæ³•æ˜¯åœ¨æ¯ä¸€è½®å»ºæ ‘æ—¶ï¼Œæ ·æœ¬æ˜¯ä»è®­ç»ƒé›†åˆä¸­æ— æ”¾å›éšæœºæŠ½æ ·çš„$\eta$éƒ¨åˆ†ï¼Œå…¸å‹çš„$\eta$å€¼æ˜¯0.5ã€‚è¿™æ ·åšæ—¢èƒ½å¯¹æ¨¡å‹èµ·æ­£åˆ™ä½œç”¨ï¼Œä¹Ÿèƒ½å‡å°‘è®¡ç®—æ—¶é—´ã€‚</p><p>äº‹å®ä¸Šï¼ŒXGBoostå’ŒSklearnçš„å®ç°å‡å€Ÿé‰´äº†éšæœºæ£®æ—ï¼Œé™¤äº†æœ‰æ ·æœ¬å±‚æ¬¡ä¸Šçš„é‡‡æ ·ï¼Œä¹Ÿæœ‰ç‰¹å¾é‡‡æ ·ã€‚ä¹Ÿå°±æ˜¯è¯´å»ºæ ‘çš„æ—¶å€™åªä»éšæœºé€‰å–çš„ä¸€äº›ç‰¹å¾åˆ—å¯»æ‰¾æœ€ä¼˜åˆ†è£‚ã€‚<br>ä¸‹é¢æ˜¯Sklearné‡Œçš„ç›¸å…³å‚æ•°è®¾ç½®çš„ç‰‡æ®µï¼Œ</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#code snippets from sklearn.ensemble.gradient_boosting</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GradientBoostingClassifier</span><span class="params">(BaseGradientBoosting, ClassifierMixin)</span>:</span></span><br><span class="line">    <span class="string">"""Gradient Boosting for classification."""</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ..., subsample=<span class="number">1.0</span>, max_features=None,...)</span>:</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">subsample : float, optional (default=1.0)</span></span><br><span class="line"><span class="string">        The fraction of samples to be used for fitting the individual base</span></span><br><span class="line"><span class="string">        learners. If smaller than 1.0 this results in Stochastic Gradient</span></span><br><span class="line"><span class="string">        Boosting. `subsample` interacts with the parameter `n_estimators`.</span></span><br><span class="line"><span class="string">        Choosing `subsample &lt; 1.0` leads to a reduction of variance</span></span><br><span class="line"><span class="string">        and an increase in bias.</span></span><br><span class="line"><span class="string">    max_features : int, float, string or None, optional (default=None)</span></span><br><span class="line"><span class="string">        The number of features to consider when looking for the best split:</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><h3 id="Regularized-Learning-Objective"><a href="#Regularized-Learning-Objective" class="headerlink" title="Regularized Learning Objective"></a>Regularized Learning Objective</h3><p>å°†æ ‘æ¨¡å‹çš„å¤æ‚åº¦ä½œä¸ºæ­£åˆ™é¡¹æ˜¾å¼åœ°åŠ è¿›ä¼˜åŒ–ç›®æ ‡é‡Œï¼Œæ˜¯XGBoostå®ç°çš„ç‹¬åˆ°ä¹‹å¤„ã€‚<br>$$\mathcal{L}^{(t)} = \sum_{i=1}^n l(y_i, y_i^{*(t-1)} + f_t(\mathrm{x}_i)) + \Omega(f_t)$$<br>where<br>$$ \Omega(f)=\gamma T+ \frac{1}{2}\lambda||w||^2$$</p><p>å…¶ä¸­$y_i^{*(t)}$æ˜¯ç¬¬tè½®ç¬¬iä¸ªinstanceçš„é¢„æµ‹å€¼ï¼Œ$f_t$æ˜¯ç¬¬tè½®å»ºçš„æ ‘ï¼Œ$T$æ˜¯æ ‘å¶ç»“ç‚¹æ•°ç›®ï¼Œ$w$æ˜¯æ ‘å¶ç»“ç‚¹çš„è¾“å‡ºï¼Œ$\gamma, \lambda$æ˜¯æ­£åˆ™åŒ–å‚æ•°ã€‚æ·±å…¥äº†è§£åŠ äº†æ­£åˆ™åå¦‚ä½•æ¨å¯¼å‰ƒåº¦æ›´æ–°çš„å¯ä»¥çœ‹XGBoostçš„<a href="http://arxiv.org/abs/1603.02754" target="_blank" rel="noopener">è®ºæ–‡</a>ã€‚</p><p>æˆ‘ä¸ªäººçš„çœ‹æ³•æ˜¯å°†æ ‘æ¨¡å‹çš„å¤æ‚åº¦ä½œä¸ºæ­£åˆ™åŒ–é¡¹åŠ åœ¨ä¼˜åŒ–ç›®æ ‡ï¼Œç›¸æ¯”è‡ªå·±é€šè¿‡å‚æ•°æ§åˆ¶æ¯è½®æ ‘çš„å¤æ‚åº¦æ›´ç›´æ¥ï¼Œè¿™å¯èƒ½æ˜¯XGBoostç›¸æ¯”æ™®é€šGBDTå®ç°æ•ˆæœæ›´å¥½çš„ä¸€ä¸ªå¾ˆé‡è¦çš„åŸå› ã€‚å¾ˆé—æ†¾ï¼ŒSklearnæš‚æ—¶æ— ç›¸åº”çš„å®ç°ã€‚</p><h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>Dropoutæ˜¯deep learningé‡Œå¾ˆå¸¸ç”¨çš„æ­£åˆ™åŒ–æŠ€å·§ï¼Œå¾ˆè‡ªç„¶çš„æˆ‘ä»¬ä¼šæƒ³èƒ½ä¸èƒ½æŠŠDropoutç”¨åˆ°GBDTæ¨¡å‹ä¸Šå‘¢ï¼Ÿ<a href="http://www.jmlr.org/proceedings/papers/v38/" target="_blank" rel="noopener">AISTATS2015</a>æœ‰ç¯‡æ–‡ç« [DART: Dropouts meet Multiple Additive Regression Trees](DART: Dropouts meet Multiple Additive Regression Trees)è¿›è¡Œäº†ä¸€äº›å°è¯•ã€‚</p><p>æ–‡ä¸­æåˆ°GBDTé‡Œä¼šå‡ºç°<em>over-specialization</em>çš„é—®é¢˜ï¼š</p><blockquote><p>Trees added at later iterations tend to impact the prediction of only<br>a few instances, and they make negligible contribution<br>towards the prediction of all the remaining instances.<br>We call this issue of subsequent trees affecting the prediction of only<br>a small fraction of the training instances <em>over-specialization</em>.</p></blockquote><p>ä¹Ÿå°±æ˜¯è¯´å‰é¢è¿­ä»£çš„æ ‘å¯¹é¢„æµ‹å€¼çš„è´¡çŒ®æ¯”è¾ƒå¤§ï¼Œåé¢çš„æ ‘ä¼šé›†ä¸­é¢„æµ‹ä¸€å°éƒ¨åˆ†æ ·æœ¬çš„åå·®ã€‚Shrinkageå¯ä»¥å‡è½»<em>over-specialization</em>çš„é—®é¢˜ï¼Œä½†ä¸æ˜¯å¾ˆå¥½ã€‚ä½œè€…æƒ³é€šè¿‡Dropoutæ¥å¹³è¡¡æ‰€æœ‰æ ‘å¯¹é¢„æµ‹çš„è´¡çŒ®ï¼Œå¦‚ä¸‹å›¾çš„æ•ˆæœï¼š<br><img src="/assets/images/gbdt_dart.png" alt=""></p><p>å…·ä½“çš„åšæ³•å¦‚ä¸‹ï¼š</p><blockquote><p>DART divergesfrom MART at two places. First, when computing the<br>gradient that the next tree will fit, only a random subset<br>of the existing ensemble is considered.<br>The second place at which DART diverges from MART<br>is when adding the new tree to the ensemble where<br>DART performs a normalization step.</p></blockquote><p>ç®€å•è¯´å°±æ˜¯æ¯æ¬¡æ–°åŠ ä¸€æ£µæ ‘ï¼Œè¿™æ£µæ ‘è¦æ‹Ÿåˆçš„å¹¶ä¸æ˜¯ä¹‹å‰å…¨éƒ¨æ ‘ensembleåçš„æ®‹å·®ï¼Œè€Œæ˜¯éšæœºæŠ½å–çš„ä¸€äº›æ ‘ensembleï¼›åŒæ—¶æ–°åŠ çš„æ ‘ç»“æœè¦è§„èŒƒåŒ–ä¸€ä¸‹ã€‚</p><p>è¿™ç§æ–°åšæ³•å¯¹GBDTæ•ˆæœçš„æå‡æœ‰å¤šæ˜æ˜¾è¿˜æœ‰å¾…å¤§å®¶æ¢ç´¢å°è¯•ã€‚</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script src=&quot;//gist.github.com/930f5f2f2b315ec967617d88600f16df.js&quot;&gt;&lt;/script&gt;


&lt;p&gt;ä¹‹å‰ä¸€ç¯‡æ–‡ç« ç®€å•åœ°è®²äº†XGBoostçš„å®ç°ä¸æ™®é€šGBDTå®ç°çš„ä¸åŒä¹‹å¤„ï¼Œæœ¬æ–‡å°è¯•æ€»ç»“ä¸€ä¸‹GBDTè¿ç”¨çš„æ­£åˆ™åŒ–æŠ€å·§ã€‚
      
    
    </summary>
    
    
      <category term="æœºå™¨å­¦ä¹ " scheme="http://yixuxi.xyz/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>how to learn golang</title>
    <link href="http://yixuxi.xyz/posts/yuque/how%20to%20learn%20golang/"/>
    <id>http://yixuxi.xyz/posts/yuque/how to learn golang/</id>
    <published>2019-11-14T00:36:18.000Z</published>
    <updated>2019-11-14T08:40:44.739Z</updated>
    
    <content type="html"><![CDATA[<ul><li>go.dev</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;go.dev&lt;/li&gt;
&lt;/ul&gt;

      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>åšå®¢ç¬¬1æœŸ</title>
    <link href="http://yixuxi.xyz/posts/yuque/%E5%8D%9A%E5%AE%A2%E7%AC%AC1%E6%9C%9F/"/>
    <id>http://yixuxi.xyz/posts/yuque/åšå®¢ç¬¬1æœŸ/</id>
    <published>2019-11-12T22:23:04.000Z</published>
    <updated>2019-11-14T08:40:44.743Z</updated>
    
    <content type="html"><![CDATA[<p>è¯¥åšå®¢æ¨¡ç‰ˆæ¡ˆä¾‹æ¥è‡ª <a href="#">@èš‚èšé‡‘æœä½“éªŒæŠ€æœ¯éƒ¨</a><br><a name="Cpuzj"></a></p><h3 id=""><a href="#" class="headerlink" title=""></a></h3><p>æµ‹è¯•<br><a name="az6uF"></a></p><h3 id="æ’å…¥å¸¦é“¾æ¥çš„æ ‡é¢˜ï¼ˆctrl-Kï¼‰Best-Year-in-Music"><a href="#æ’å…¥å¸¦é“¾æ¥çš„æ ‡é¢˜ï¼ˆctrl-Kï¼‰Best-Year-in-Music" class="headerlink" title="æ’å…¥å¸¦é“¾æ¥çš„æ ‡é¢˜ï¼ˆctrl+Kï¼‰Best Year in Music"></a>æ’å…¥å¸¦é“¾æ¥çš„æ ‡é¢˜ï¼ˆctrl+Kï¼‰<a href="https://pudding.cool/projects/music-history/" target="_blank" rel="noopener">Best Year in Music</a></h3><p>è¿™å¹…å¯è§†åŒ–ä½œå“å±•ç¤ºäº† 1960 å¹´ä»¥æ¥ï¼ŒBillboard å†å¹´æ¦œå•çš„å‰äº”åéŸ³ä¹çš„å˜åŒ–æƒ…å†µï¼Œæ•ˆæœå‘ˆç°è®¾è®¡æ„Ÿåè¶³ï¼Œè¿˜å¯ä»¥åœ¨è†å¬éŸ³ä¹çš„åŒæ—¶å›é¡¾éŸ³ä¹çš„å˜è¿å²ã€‚<br /><img src="https://cdn.nlark.com/yuque/0/2019/png/85075/1569743511172-7dfff3da-684a-4902-babc-cd9117fe3b7a.png#align=left&display=inline&height=648&name=image.png&originHeight=1296&originWidth=2868&search=&size=965213&status=done&width=1434" alt="image.png"><br /><em>æ’å…¥å›¾ç‰‡ï¼Œç‚¹å‡»å›¾ç‰‡ï¼Œè®¾ç½®å›¾ç‰‡å¤§å°</em><br />å€¼å¾—ä¸€æçš„æ˜¯ï¼Œä½œè€…æ­£æ˜¯å¤§åé¼é¼çš„æ•°æ®æ–°é—»ç«™ç‚¹ <a href="https://pudding.cool/" target="_blank" rel="noopener">The Pudding</a>ã€‚ä½œä¸ºå¯è§†åŒ–æ–°é—»åª’ä½“ä¸­çš„å§£å§£è€…ï¼ŒThe Pudding æ“…é•¿å¯¹é‚£äº›å…¼å…·ä¿¡æ¯é‡å’Œå¨±ä¹åº¦çš„äº‰è®®æ€§è¯é¢˜è¿›è¡Œå¯è§†åŒ–å‘ˆç°ï¼Œæ¥ä½¿å¾—å¤æ‚çš„è§‚ç‚¹å˜å¾—æ›´å®¹æ˜“è¢«å…¬ä¼—ç†è§£ã€‚è¯¥å›¢é˜Ÿè™½ç„¶ä»…ç”±æ•°åå…¨èŒè®°è€…å’Œå·¥ç¨‹å¸ˆç»„æˆï¼Œä½†æˆ˜æ–—åŠ›å¼ºæ‚ï¼Œç›®å‰å·²æœ‰æ•°åä¸ªå¤§å‹çš„æ•°æ®å¯è§†åŒ–ä½œå“ã€‚<br />è¾“å…¥ä½œè€… <a href="/dengfuping">@è¯¸å²³(dengfuping)</a><br /><br><br /></p><p><a name="xyPrr"></a></p><h3 id="Road-Suffixes-in-the-USA"><a href="#Road-Suffixes-in-the-USA" class="headerlink" title="Road Suffixes in the USA"></a><a href="https://erdavis.com/2019/07/04/road-suffixes-in-the-usa-take-2/" target="_blank" rel="noopener">Road Suffixes in the USA</a></h3><p>ç¾å›½é“è·¯å‘½åå¤šç§å¤šæ ·ï¼Œä¾‹å¦‚è‘—åçš„çº½çº¦ç¬¬äº”å¤§é“ï¼ˆFifth Avenueï¼‰ï¼Œåˆæˆ–è€…å‡¤å‡°åŸï¼ˆPhoenixï¼‰çš„Washington Streetï¼ŒThomas Roadï¼ŒUnion Hill Driveç­‰ã€‚ä¸åŒçš„é“è·¯ï¼Œæœ‰ä¸åŒçš„å«æ³•ï¼ŒAvenue, Street, Road, Driveç­‰ç­‰ã€‚æƒ³äº†è§£æ›´å¤šå…³äºç¾å›½é“è·¯å‘½åå¯ä»¥å‚è€ƒ<a href="http://blog.sina.com.cn/s/blog_7010d1db0101he93.html" target="_blank" rel="noopener">è¿™ç¯‡æ–‡ç« </a>ã€‚<br />ä½œè€…ç”¨Rè¯­è¨€ç»Ÿè®¡äº†ç¾å›½æ¯ä¸ªå¿çš„æ¯ç§é“è·¯åç¼€ï¼ˆä¾‹å¦‚Avenueï¼‰çš„æ€»é‡Œç¨‹ï¼Œå¹¶æŠŠæ¯ä¸ªå¿æ€»é‡Œç¨‹æœ€å¤§çš„é“è·¯åç¼€é€šè¿‡é¢œè‰²æ˜ å°„åˆ°åœ°å›¾ä¸Šã€‚å¯ä»¥çœ‹å‡ºç¾å›½å¤§éƒ¨åˆ†å¿çš„é“è·¯éƒ½æ˜¯ä»¥Rd(Roadçš„ç¼©å†™)ç»“å°¾çš„ã€‚<br /><img src="https://cdn.nlark.com/yuque/0/2019/png/85075/1569743625739-f1c8b376-5632-4d08-8fcc-56142f0b0c1a.png#align=left&display=inline&height=603&name=road-suffix-map.png&originHeight=1767&originWidth=2048&search=&size=328604&status=done&width=699" alt="road-suffix-map.png"><br /><em>æ’å…¥å›¾ç‰‡ï¼Œç‚¹å‡»å›¾ç‰‡ï¼Œè®¾ç½®å›¾ç‰‡å¤§å°</em></p><p>å¦å¤–è¿˜ä»¥æŸ±çŠ¶å›¾çš„æ–¹å¼å±•ç¤ºäº†æ¯ç§é“è·¯åç¼€çš„æ€»é‡Œç¨‹ï¼ˆè‹±é‡Œï¼‰ï¼š<br /><img src="https://cdn.nlark.com/yuque/0/2019/png/85075/1569743625720-b8759449-4b6a-4b11-bbc0-896fe8dca73e.png#align=left&display=inline&height=737&name=road-suffix-bar.png&originHeight=737&originWidth=890&search=&size=23572&status=done&width=890" alt="road-suffix-bar.png"><br />by <a href="/changzhe">@é•¿å“²(changzhe)</a></p><p><a name="hduVH"></a></p><h1 id="ã€ŒåŠ é¤ã€Hello-World-å›¾å¯è§†åŒ–"><a href="#ã€ŒåŠ é¤ã€Hello-World-å›¾å¯è§†åŒ–" class="headerlink" title="ã€ŒåŠ é¤ã€Hello World å›¾å¯è§†åŒ–"></a><a href="https://zhuanlan.zhihu.com/p/83685690" target="_blank" rel="noopener">ã€Œ</a><a href="https://zhuanlan.zhihu.com/p/83685690" target="_blank" rel="noopener">åŠ é¤ã€</a><a href="https://zhuanlan.zhihu.com/p/83685690" target="_blank" rel="noopener">Hello World å›¾å¯è§†åŒ–</a></h1><p>å›¾å¯è§†åŒ–æ˜¯ä¿¡æ¯å¯è§†åŒ–çš„å­é¢†åŸŸï¼Œå®ƒé€šè¿‡å±•ç¤ºå…ƒç´ ã€å…³ç³»ï¼Œå¸®åŠ©ç”¨æˆ·è·å–æ•°æ®çš„æ´æ‚‰èƒ½åŠ›ã€‚å®ƒå·²è¢«å¹¿æ³›åœ°åº”ç”¨åœ¨æµç¨‹å›¾ã€ç¤¾äº¤ç½‘ç»œã€è‹±ç‰¹ç½‘ã€è›‹ç™½è´¨ç½‘ç»œç­‰å…³ç³»æ•°æ®çš„å‘ˆç°ã€‚<br /><a href="https://zhuanlan.zhihu.com/p/83685690" target="_blank" rel="noopener">æœ¬æ–‡</a>ç”± AntV å›¢é˜Ÿ <a href="/shiwu-5wap2">@åå¾(shiwu-5wap2)</a> ğŸ‘†æ’°å†™ï¼Œå‚è€ƒè‡ª[1][2]ä¸¤ç¯‡å›¾å¯è§†åŒ–å­¦æœ¯è®ºæ–‡ï¼Œç®€è¦ä»‹ç»å›¾å¯è§†åŒ–çš„å†å²ã€èƒŒæ™¯ã€æœºé‡ä¸æŒ‘æˆ˜ã€‚</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;è¯¥åšå®¢æ¨¡ç‰ˆæ¡ˆä¾‹æ¥è‡ª &lt;a href=&quot;#&quot;&gt;@èš‚èšé‡‘æœä½“éªŒæŠ€æœ¯éƒ¨&lt;/a&gt;&lt;br&gt;&lt;a name=&quot;Cpuzj&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;&quot;&gt;&lt;a href=&quot;#&quot; class=&quot;headerlink&quot; title=&quot;&quot;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;æµ‹è¯•&lt;br&gt;&lt;a n
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yixuxi.xyz/posts/hello-world/"/>
    <id>http://yixuxi.xyz/posts/hello-world/</id>
    <published>2016-04-05T06:16:00.000Z</published>
    <updated>2019-11-14T08:40:04.315Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
      <category term="math" scheme="http://yixuxi.xyz/categories/math/"/>
    
    
  </entry>
  
</feed>

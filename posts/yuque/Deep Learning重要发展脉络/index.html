<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta name="referrer" content="no-referrer"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Deep Learning重要发展脉络 | 噫吁戏</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script async src="https://www.googletagmanager.com/gtag/js"></script><script>var tracking_ids = 'G-L3VL1KX4HZ,UA-151168174-1'.split(',');
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
for(var i=0;i<=tracking_ids.length;i++){
  gtag('config', tracking_ids[i]);
}

</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '584ba9dd631b05e9e41f2c7c71f659eb';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Deep Learning重要发展脉络</h1><a id="logo" href="/.">噫吁戏</a><p class="description">Just Do IT.</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Deep Learning重要发展脉络</h1><div class="post-meta">Dec 26, 2018<script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 1.8k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 6</span><span class="post-meta-item-text"> 分钟</span></span></span></div><a class="disqus-comment-count" data-disqus-identifier="posts/yuque/Deep Learning重要发展脉络/" href="/posts/yuque/Deep%20Learning%E9%87%8D%E8%A6%81%E5%8F%91%E5%B1%95%E8%84%89%E7%BB%9C/#disqus_thread"></a><div class="post-content"><p>Deep Learning（深度学习）的概念源于人工神经网络的研究，它的概念由Hinton等人于2006年提出，但它的模型经历了怎样的发展和演化，本文将为您深度解读Deep Learning的前世今生。<br /><img src="https://static.aminer.cn/rcd/article/expertpic/aminer.gif#align=left&display=inline&height=1205&originHeight=1205&originWidth=1694&search=&status=done&width=1694" alt=""><br /><br><a name="tfacwu"></a></p>
<h3 id="脉络一-cv-tensor"><a href="#脉络一-cv-tensor" class="headerlink" title="脉络一  cv/tensor"></a>脉络一  cv/tensor</h3><p>1943年  心理学家麦卡洛可（W·McCulloch）和数理逻辑学家皮茨（W·Pitts）参考了生物神经元的结构，发表了《神经活动中思想内在性的逻辑运算》一文，提出了抽象的神经元模型MP，该模型可以看做深度学习的雏形。</p>
<p>1957年  认知心理学大师 Frank Rosenblatt 发明了感知机（Perceptron，又称感知器），感知机是当时首个可以学习的人工神经网络，掀起了一股学习的热潮。</p>
<p>1969年  人工智能大师 Marvin Minksy 和 Seymour Papert 在《Perceptron》一书中，用详细的数学证明了感知机的弱点，没有隐层的简单感知机在许多像XOR问题的情形下显得无能为力，并证明了简单感知机只能解决线性分类问题和一阶谓诃同题。神经网络研究进入冰河期。</p>
<p>1984年  日本学者福岛邦彦（Kunihiko Fukishima）提出了卷积神经网络的原始模型神经感知机（Neocognitron），产生了卷积和池化的思想（当时不叫卷积和池化）。</p>
<p>1986年  Hinton等人正式提出一般 Delta 法则，即反向传播（BP）算法，并用反向传播训练MLP（多层感知机）。但其实在他提出之前，已经有人将其付诸实际。（1985年 Parter 也独立地得出过相似的算法，他称之为学习逻辑。此外，1985年 Lecun 也研究出大致相似的学习法则。）</p>
<p>1998年  以 Yann LeCun 为首的研究人员实现了一个5层的卷积神经网络——LeNet-5，以识别手写数字。LeNet-5 标志着 CNN（卷积神经网络）的真正面世，LeNet-5 的提出把 CNN 推上了一个小高潮。</p>
<p>之后SVM（支持向量机）兴起，SVM在计算及准确度上都有较大的优势，导致卷积神经网络的方法在后来的一段时间并未能火起来。</p>
<p>2012年  Hinton组的 AlexNet 在 ImageNet 上以巨大优势夺冠，掀起了深度学习的热潮。AlexNet 可以算是 LeNet 的一种更深更宽的版本，并加上了 relu、dropout 等技巧。</p>
<p>这条思路被后人发展，出现了 VGG，GoogLeNet 等网络。</p>
<p>2016年  青年计算机视觉科学家何恺明在层次之间加入了跳跃连接，Resnet 极大增加了网络深度，效果有很大提升。另一个将这条思路继续发展下去的是去年cvpr best paper densenet。</p>
<p>除此之外，cv领域的特定任务还出现了各种各样的模型（Mask-RCNN等），这里不一一介绍。</p>
<p>2017年  Hinton认为反省传播和传统神经网络有缺陷，继而提出了 Capsule Net。但是目前在 cifar 等数据集上效果一般，这个思路还需要继续验证和发展。</p>
<p><a name="orhvmd"></a></p>
<h3 id="脉络二-生成模型"><a href="#脉络二-生成模型" class="headerlink" title="脉络二  生成模型"></a>脉络二  生成模型</h3><p>传统的生成模型是要预测联合概率分布P(x,y)。</p>
<p>RBM（受限玻尔兹曼机）这个模型其实是一个基于能量的模型，1986年的时候就有，2006年将其重新拿出来作为一个生成模型，并且堆叠成为deep belief network，使用逐层贪婪或者wake-sleep的方法训练，不过这个模型效果一般，现在已经没什么人提了。但是Hinton等人却从此开始使用深度学习重新包装神经网络。</p>
<p>Auto-Encoder是上个世纪80年代hinton提出的模型，如今由于计算能力的进步重新登上舞台。2008年，Bengio等人又搞了denoise Auto-Encoder。</p>
<p>Max welling等人使用神经网络训练一个有一层隐变量的图模型，由于使用了变分推断，最后长得跟auto-encoder有点像，因而被称为Variational auto-encoder。此模型可以通过隐变量的分布采样，经过后面的decoder网络直接生成样本。</p>
<p>GAN（生成对抗网络）是于2014年提出的模型，如今炙手可热。它是一个生成模型，通过判别器D（Discriminator）和生成器G（Generator）的对抗训练，直接使用神经网络G隐式建模样本整体的概率分布。每次运行便相当于从分布中采样。</p>
<p>DCGAN是一个相当好的卷积神经网络实现，而WGAN则是通过维尔斯特拉斯距离替换原来的JS散度来度量分布之间的相似性的工作，使得训练稳定。PGGAN则逐层增大网络，生成机器逼真的人脸。</p>
<p><a name="rkublv"></a></p>
<h3 id="脉络三-sequencelearning"><a href="#脉络三-sequencelearning" class="headerlink" title="脉络三 sequencelearning"></a>脉络三 sequencelearning</h3><p>1982年  出现的hopfield network有了递归网络的思想。</p>
<p>1997年  Jürgen Schmidhuber发明LSTM，并做了一系列的工作。但是更有影响力的还是2013年由Hinton组使用RNN做的语音识别工作，这种方法比传统方法更强。</p>
<p>文本方面，Bengio在svm最火的时期提出了一种基于神经网络的语言模型，后来Google提出的word2vec也包含了一些反向传播的思想。在机器翻译等任务上，逐渐出现了以RNN为基础的seq2seq模型（序列模型），模型通过一个encoder（编码器）把一句话的语义信息压成向量再通过decoder（解码器）输出，当然更多的还要和Attention Model（注意力模型）结合。</p>
<p>后来，大家发现使用以字符为单位的CNN模型在很多语言任务也有不俗的表现，而且时空消耗更少。LSTM/RNN 模型中的Attention机制是用于克服传统编码器-解码器结构存在的问题的。其中，self-attention（自注意力机制）实际上就是采取一种结构去同时考虑同一序列局部和全局的信息，Google就有一篇耸人听闻的Attention Is All You Need的文章。</p>
<p><a name="65uhmf"></a></p>
<h3 id="脉络四-deepreinforcement-learning"><a href="#脉络四-deepreinforcement-learning" class="headerlink" title="脉络四 deepreinforcement learning"></a>脉络四 deepreinforcement learning</h3><p>该领域最出名的是DeepMind，这里列出的David Silver则是一直研究rl（强化学习）的高管。</p>
<p>q-learning是很有名的传统rl算法，deep q-learning则是将原来的q值表用神经网络代替。利用deep q-learning制作的打砖块的游戏十分有名。后来David Silver等人又利用其测试了许多游戏，发在了Nature上。</p>
<p>增强学习在double duel的进展，主要是Qlearning的权重更新时序上。</p>
<p>DeepMind的其他工作诸如DDPG、A3C也非常有名，它们是基于policy gradient和神经网络结合的变种。</p>
<p>大家都知道的一个应用是AlphaGo，里面不仅使用了rl的方法，也包含了传统的蒙特卡洛搜索技巧。Alpha Zero 则是他们搞了一个用Alphago框架来打其他棋类游戏的游戏，而且这个“打”还是吊打的打。</p>
</div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>zhiyue</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/posts/yuque/Deep Learning重要发展脉络/">http://yixuxi.xyz/posts/yuque/Deep%20Learning%E9%87%8D%E8%A6%81%E5%8F%91%E5%B1%95%E8%84%89%E7%BB%9C/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均采用 CC BY-NC-SA 3.0 CN 许可协议。转载请注明出处！</li></ul></div><br><div class="tags"></div><div class="post-nav"><a class="pre" href="/posts/yuque/%E6%97%A0%E6%A0%87%E9%A2%98/">无标题</a><a class="next" href="/posts/yuque/Elasticsearch%20%E7%9F%A5%E8%AF%86%E7%82%B9%E7%B4%A2%E5%BC%95/">Elasticsearch 知识点索引</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论（请确保 Disqus 可以正常加载）</button></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'http://yixuxi.xyz/posts/yuque/Deep Learning重要发展脉络/';
    this.page.identifier = 'posts/yuque/Deep Learning重要发展脉络/';
    this.page.title = 'Deep Learning重要发展脉络';
  };</script><script type="text/javascript" id="disqus-lazy-load-script">$.ajax({
url: 'https://disqus.com/next/config.json',
timeout: 2500,
type: 'GET',
success: function(){
  var d = document;
  var s = d.createElement('script');
  s.src = '//zhiyue.disqus.com/embed.js';
  s.setAttribute('data-timestamp', + new Date());
  (d.head || d.body).appendChild(s);
  $('.disqus_click_btn').css('display', 'none');
},
error: function() {
  $('.disqus_click_btn').css('display', 'block');
}
});</script><script type="text/javascript" id="disqus-click-load">$('.btn_click_load').click(() => {  //click to load comments
    (() => { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//zhiyue.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
    $('.disqus_click_btn').css('display','none');
});</script><script type="text/javascript" id="disqus-count-script">$(function() {
     var xhr = new XMLHttpRequest();
     xhr.open('GET', '//disqus.com/next/config.json', true);
     xhr.timeout = 2500;
     xhr.onreadystatechange = function () {
       if (xhr.readyState === 4 && xhr.status === 200) {
         $('.post-meta .post-comments-count').show();
         var s = document.createElement('script');
         s.id = 'dsq-count-scr';
         s.src = 'https://zhiyue.disqus.com/count.js';
         s.async = true;
         (document.head || document.body).appendChild(s);
       }
     };
     xhr.ontimeout = function () { xhr.abort(); };
     xhr.send(null);
   });
</script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://yixuxi.xyz"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/math/">math</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/posts/hello-world_2/">Regularization on GBDT</a></li><li class="post-list-item"><a class="post-list-link" href="/posts/yuque/%E5%8D%9A%E5%AE%A2%E7%AC%AC1%E6%9C%9F/">博客第1期</a></li><li class="post-list-item"><a class="post-list-link" href="/posts/yuque/%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/">如何学习数据科学</a></li><li class="post-list-item"><a class="post-list-link" href="/posts/yuque/%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0%20pandas/">如何学习 pandas</a></li><li class="post-list-item"><a class="post-list-link" href="/posts/yuque/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%20%7C%20SF-Zhou's%20Blog/">《机器学习》西瓜书阅读笔记 | SF-Zhou&#39;s Blog</a></li><li class="post-list-item"><a class="post-list-link" href="/posts/yuque/%E5%A4%A7%E5%AD%A6%E8%AF%BE%E7%A8%8B/">大学课程</a></li><li class="post-list-item"><a class="post-list-link" href="/posts/yuque/%E4%BA%8B%E5%AE%9E%E4%B8%8A/">事实上</a></li><li class="post-list-item"><a class="post-list-link" href="/posts/yuque/%E5%A6%82%E4%BD%95%E7%A7%91%E5%AD%A6%E5%87%8F%E8%82%A5/">如何科学减肥</a></li><li class="post-list-item"><a class="post-list-link" href="/posts/yuque/%E6%97%A0%E6%A0%87%E9%A2%98/">无标题</a></li><li class="post-list-item"><a class="post-list-link" href="/posts/yuque/Deep%20Learning%E9%87%8D%E8%A6%81%E5%8F%91%E5%B1%95%E8%84%89%E7%BB%9C/">Deep Learning重要发展脉络</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-comment-o"> 最近评论</i></div><script type="text/javascript" src="//zhiyue.disqus.com/recent_comments_widget.js?num_items=5&amp;hide_avatars=1&amp;avatar_size=32&amp;excerpt_length=20&amp;hide_mods=1"></script></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">噫吁戏.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>
<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta name="referrer" content="no-referrer"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Regularization on GBDT | 噫吁戏</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script async src="https://www.googletagmanager.com/gtag/js"></script><script>var tracking_ids = 'G-L3VL1KX4HZ,UA-151168174-1'.split(',');
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
for(var i=0;i<=tracking_ids.length;i++){
  gtag('config', tracking_ids[i]);
}

</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '584ba9dd631b05e9e41f2c7c71f659eb';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Regularization on GBDT</h1><a id="logo" href="/.">噫吁戏</a><p class="description">Just Do IT.</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Regularization on GBDT</h1><div class="post-meta">Nov 14, 2019<span> | </span><span class="category"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 1.5k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 6</span><span class="post-meta-item-text"> 分钟</span></span></span></div><a class="disqus-comment-count" data-disqus-identifier="posts/hello-world_2/" href="/posts/hello-world_2/#disqus_thread"></a><div class="post-content"><script src="//gist.github.com/930f5f2f2b315ec967617d88600f16df.js"></script>


<p>之前一篇文章简单地讲了XGBoost的实现与普通GBDT实现的不同之处，本文尝试总结一下GBDT运用的正则化技巧。</p>
<script src="https://gist.github.com/zihengcat/1a80a31b671e5bb3db6eb7af5a4b30f7.js"></script>

<h3 id="Early-Stopping"><a href="#Early-Stopping" class="headerlink" title="Early Stopping"></a>Early Stopping</h3><p><a href="https://en.wikipedia.org/wiki/Early_stopping" target="_blank" rel="noopener">Early Stopping</a>是机器学习迭代式训练模型中很常见的防止过拟合技巧，维基百科里如下描述:</p>
<blockquote>
<p>In machine learning, early stopping is a form of <em>regularization</em> used to <em>avoid overfitting</em> when training a learner with an <em>iterative method</em>, such as gradient descent.</p>
</blockquote>
<p>具体的做法是选择一部分样本作为验证集，在迭代拟合训练集的过程中，如果模型在验证集里错误率不再下降，就停止训练，也就是说控制迭代的轮数（树的个数）。</p>
<p>XGBoost Python关于early stopping的<a href="https://github.com/dmlc/xgboost/blob/master/doc/python/python_intro.md#early-stopping" target="_blank" rel="noopener">参数设置文档</a>非常清晰，API如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># code snippets from xgboost python-package training.py</span><br><span class="line">def train(..., evals=(), early_stopping_rounds=None)</span><br><span class="line">	&quot;&quot;&quot;Train a booster with given parameters.</span><br><span class="line">	Parameters</span><br><span class="line">    ----------</span><br><span class="line">	early_stopping_rounds: int</span><br><span class="line">        Activates early stopping. Validation error needs to decrease at least</span><br><span class="line">        every &lt;early_stopping_rounds&gt; round(s) to continue training.</span><br><span class="line">	&quot;&quot;&quot;</span><br></pre></td></tr></table></figure>


<p>Sklearn的GBDT实现虽然可以添加early stopping，但是比较复杂。官方没有相应的文档和代码样例，必须看<a href="https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/gradient_boosting.py#L931" target="_blank" rel="noopener">源码</a>。实现的时候需要用户提供monitor回调函数，且要了解源码内部_fit_stages函数的locals，总之对新手很不友好：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#code snippets from sklearn.ensemble.gradient_boosting</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaseGradientBoosting</span><span class="params">(six.with_metaclass<span class="params">(ABCMeta, BaseEnsemble,</span></span></span></span><br><span class="line"><span class="class"><span class="params"><span class="params">						   _LearntSelectorMixin)</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Abstract base class for Gradient Boosting. """</span></span><br><span class="line">	...</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y, sample_weight=None, monitor=None)</span>:</span></span><br><span class="line">		<span class="string">"""Fit the gradient boosting model.</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">		----------</span></span><br><span class="line"><span class="string">		monitor : callable, optional</span></span><br><span class="line"><span class="string">            The monitor is called after each iteration with the current</span></span><br><span class="line"><span class="string">            iteration, a reference to the estimator and the local variables of</span></span><br><span class="line"><span class="string">            ``_fit_stages`` as keyword arguments ``callable(i, self,</span></span><br><span class="line"><span class="string">            locals())``. If the callable returns ``True`` the fitting procedure</span></span><br><span class="line"><span class="string">            is stopped. The monitor can be used for various things such as</span></span><br><span class="line"><span class="string">            computing held-out estimates, early stopping, model introspect, and</span></span><br><span class="line"><span class="string">            snapshoting.</span></span><br><span class="line"><span class="string">		"""</span></span><br></pre></td></tr></table></figure>
<p>对Sklearn感兴趣的可以看这篇文章<a href="*https://henri.io/posts/using-gradient-boosting-with-early-stopping.html">Using Gradient Boosting (with Early Stopping)</a>，里面有回调函数monitor的参考实现。</p>
<h3 id="Shrinkage"><a href="#Shrinkage" class="headerlink" title="Shrinkage"></a>Shrinkage</h3><p>Shrinkage就是将每棵树的输出结果乘一个因子($0&lt;\nu&lt;1$)，其中$\Sigma_{j=1}^{J_m}\gamma_{jm}I(x\in R_{jm})$是第m棵的输出，而$f(m)$是前m棵树的ensemble: $$f_m(x) = f_{m-1}(x) + \nu\cdot\Sigma_{j=1}^{J_m}\gamma_{jm}I(x\in R_{jm})$$<br>ESL书中这样讲：</p>
<blockquote>
<p>The parameter $\nu$ can be regarded as controlling the leanring rate of the boosting procedure</p>
</blockquote>
<p>$\nu$和迭代轮数M(树个数)是一个tradeoff，推荐的是$\nu$值设置小一点(如0.1)，而M设置大一些。这样一般能有比较好的准确率，代价是训练时间变长(与M成比例)。</p>
<p>下面是Sklearn的实现关于该参数设置的片段，XGBoost类似：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#code snippets from sklearn.ensemble.gradient_boosting</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GradientBoostingClassifier</span><span class="params">(BaseGradientBoosting, ClassifierMixin)</span>:</span></span><br><span class="line">    <span class="string">"""Gradient Boosting for classification."""</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ..., learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">100</span>, ...)</span>:</span></span><br><span class="line">	<span class="string">"""</span></span><br><span class="line"><span class="string">	Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">	learning_rate : float, optional (default=0.1)</span></span><br><span class="line"><span class="string">        learning rate shrinks the contribution of each tree by `learning_rate`.</span></span><br><span class="line"><span class="string">        There is a trade-off between learning_rate and n_estimators.</span></span><br><span class="line"><span class="string">	n_estimators : int (default=100)</span></span><br><span class="line"><span class="string">        The number of boosting stages to perform. Gradient boosting</span></span><br><span class="line"><span class="string">        is fairly robust to over-fitting so a large number usually</span></span><br><span class="line"><span class="string">        results in better performance</span></span><br><span class="line"><span class="string">	"""</span></span><br></pre></td></tr></table></figure>

<h3 id="Subsampling"><a href="#Subsampling" class="headerlink" title="Subsampling"></a>Subsampling</h3><p>Subsampling其实源于bootstrap averaging(bagging)思想，GBDT里的做法是在每一轮建树时，样本是从训练集合中无放回随机抽样的$\eta$部分，典型的$\eta$值是0.5。这样做既能对模型起正则作用，也能减少计算时间。</p>
<p>事实上，XGBoost和Sklearn的实现均借鉴了随机森林，除了有样本层次上的采样，也有特征采样。也就是说建树的时候只从随机选取的一些特征列寻找最优分裂。<br>下面是Sklearn里的相关参数设置的片段，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#code snippets from sklearn.ensemble.gradient_boosting</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GradientBoostingClassifier</span><span class="params">(BaseGradientBoosting, ClassifierMixin)</span>:</span></span><br><span class="line">    <span class="string">"""Gradient Boosting for classification."""</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ..., subsample=<span class="number">1.0</span>, max_features=None,...)</span>:</span></span><br><span class="line">	<span class="string">"""</span></span><br><span class="line"><span class="string">	Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">	subsample : float, optional (default=1.0)</span></span><br><span class="line"><span class="string">        The fraction of samples to be used for fitting the individual base</span></span><br><span class="line"><span class="string">        learners. If smaller than 1.0 this results in Stochastic Gradient</span></span><br><span class="line"><span class="string">        Boosting. `subsample` interacts with the parameter `n_estimators`.</span></span><br><span class="line"><span class="string">        Choosing `subsample &lt; 1.0` leads to a reduction of variance</span></span><br><span class="line"><span class="string">        and an increase in bias.</span></span><br><span class="line"><span class="string">    max_features : int, float, string or None, optional (default=None)</span></span><br><span class="line"><span class="string">        The number of features to consider when looking for the best split:</span></span><br><span class="line"><span class="string">	"""</span></span><br></pre></td></tr></table></figure>

<h3 id="Regularized-Learning-Objective"><a href="#Regularized-Learning-Objective" class="headerlink" title="Regularized Learning Objective"></a>Regularized Learning Objective</h3><p>将树模型的复杂度作为正则项显式地加进优化目标里，是XGBoost实现的独到之处。<br>$$\mathcal{L}^{(t)} = \sum_{i=1}^n l(y_i, y_i^{*(t-1)} + f_t(\mathrm{x}_i)) + \Omega(f_t)$$<br>where<br>$$ \Omega(f)=\gamma T+ \frac{1}{2}\lambda||w||^2$$</p>
<p>其中$y_i^{*(t)}$是第t轮第i个instance的预测值，$f_t$是第t轮建的树，$T$是树叶结点数目，$w$是树叶结点的输出，$\gamma, \lambda$是正则化参数。深入了解加了正则后如何推导剃度更新的可以看XGBoost的<a href="http://arxiv.org/abs/1603.02754" target="_blank" rel="noopener">论文</a>。</p>
<p>我个人的看法是将树模型的复杂度作为正则化项加在优化目标，相比自己通过参数控制每轮树的复杂度更直接，这可能是XGBoost相比普通GBDT实现效果更好的一个很重要的原因。很遗憾，Sklearn暂时无相应的实现。</p>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>Dropout是deep learning里很常用的正则化技巧，很自然的我们会想能不能把Dropout用到GBDT模型上呢？<a href="http://www.jmlr.org/proceedings/papers/v38/" target="_blank" rel="noopener">AISTATS2015</a>有篇文章[DART: Dropouts meet Multiple Additive Regression Trees](DART: Dropouts meet Multiple Additive Regression Trees)进行了一些尝试。</p>
<p>文中提到GBDT里会出现<em>over-specialization</em>的问题：</p>
<blockquote>
<p>Trees added at later iterations tend to impact the prediction of only<br>a few instances, and they make negligible contribution<br>towards the prediction of all the remaining instances.<br>We call this issue of subsequent trees affecting the prediction of only<br>a small fraction of the training instances <em>over-specialization</em>.</p>
</blockquote>
<p>也就是说前面迭代的树对预测值的贡献比较大，后面的树会集中预测一小部分样本的偏差。Shrinkage可以减轻<em>over-specialization</em>的问题，但不是很好。作者想通过Dropout来平衡所有树对预测的贡献，如下图的效果：<br><img src="/assets/images/gbdt_dart.png" alt=""></p>
<p>具体的做法如下：</p>
<blockquote>
<p>DART divergesfrom MART at two places. First, when computing the<br>gradient that the next tree will fit, only a random subset<br>of the existing ensemble is considered.<br>The second place at which DART diverges from MART<br>is when adding the new tree to the ensemble where<br>DART performs a normalization step.</p>
</blockquote>
<p>简单说就是每次新加一棵树，这棵树要拟合的并不是之前全部树ensemble后的残差，而是随机抽取的一些树ensemble；同时新加的树结果要规范化一下。</p>
<p>这种新做法对GBDT效果的提升有多明显还有待大家探索尝试。</p>
</div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>zhiyue</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/posts/hello-world_2/">http://yixuxi.xyz/posts/hello-world_2/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均采用 CC BY-NC-SA 3.0 CN 许可协议。转载请注明出处！</li></ul></div><br><div class="tags"></div><div class="post-nav"><a class="next" href="/posts/yuque/how%20to%20learn%20golang/">how to learn golang</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论（请确保 Disqus 可以正常加载）</button></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'http://yixuxi.xyz/posts/hello-world_2/';
    this.page.identifier = 'posts/hello-world_2/';
    this.page.title = 'Regularization on GBDT';
  };</script><script type="text/javascript" id="disqus-lazy-load-script">$.ajax({
url: 'https://disqus.com/next/config.json',
timeout: 2500,
type: 'GET',
success: function(){
  var d = document;
  var s = d.createElement('script');
  s.src = '//zhiyue.disqus.com/embed.js';
  s.setAttribute('data-timestamp', + new Date());
  (d.head || d.body).appendChild(s);
  $('.disqus_click_btn').css('display', 'none');
},
error: function() {
  $('.disqus_click_btn').css('display', 'block');
}
});</script><script type="text/javascript" id="disqus-click-load">$('.btn_click_load').click(() => {  //click to load comments
    (() => { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//zhiyue.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
    $('.disqus_click_btn').css('display','none');
});</script><script type="text/javascript" id="disqus-count-script">$(function() {
     var xhr = new XMLHttpRequest();
     xhr.open('GET', '//disqus.com/next/config.json', true);
     xhr.timeout = 2500;
     xhr.onreadystatechange = function () {
       if (xhr.readyState === 4 && xhr.status === 200) {
         $('.post-meta .post-comments-count').show();
         var s = document.createElement('script');
         s.id = 'dsq-count-scr';
         s.src = 'https://zhiyue.disqus.com/count.js';
         s.async = true;
         (document.head || document.body).appendChild(s);
       }
     };
     xhr.ontimeout = function () { xhr.abort(); };
     xhr.send(null);
   });
</script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://yixuxi.xyz"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/math/">math</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/posts/hello-world_2/">Regularization on GBDT</a></li><li class="post-list-item"><a class="post-list-link" href="/posts/yuque/how%20to%20learn%20golang/">how to learn golang</a></li><li class="post-list-item"><a class="post-list-link" href="/posts/yuque/%E5%8D%9A%E5%AE%A2%E7%AC%AC1%E6%9C%9F/">博客第1期</a></li><li class="post-list-item"><a class="post-list-link" href="/posts/hello-world/">Hello World</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-comment-o"> 最近评论</i></div><script type="text/javascript" src="//zhiyue.disqus.com/recent_comments_widget.js?num_items=5&amp;hide_avatars=1&amp;avatar_size=32&amp;excerpt_length=20&amp;hide_mods=1"></script></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">噫吁戏.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>